{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üö® IMPORTANT: Execution Order\n",
        "\n",
        "**‚ö†Ô∏è CRITICAL: Run cells in sequential order!**\n",
        "\n",
        "This notebook has dependencies between cells. If you get `NameError`, it means you skipped a prerequisite cell.\n",
        "\n",
        "## Quick Fix for Errors:\n",
        "\n",
        "If you see: `NameError: name 'X_scaled' is not defined`\n",
        "\n",
        "**Solution:** Run these cells FIRST (in order):\n",
        "1. **Cell 13**: Step 5 - Prepare Features for Training\n",
        "2. **Cell 15**: Step 6 - Train Logistic Regression  \n",
        "3. **Cell 16**: Step 6 - Train Linear SVM\n",
        "4. Then run visualization cells\n",
        "\n",
        "See `NOTEBOOK_EXECUTION_ORDER.md` for complete guide.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick Prerequisite Checker\n",
        "# Run this cell anytime to check if you have all required variables\n",
        "\n",
        "def check_prerequisites():\n",
        "    \"\"\"Check if all required variables for the notebook exist\"\"\"\n",
        "    required = {\n",
        "        'df': 'Cell 5 (Load Data)',\n",
        "        'X_scaled': 'Cell 13 (Prepare Features)',\n",
        "        'y': 'Cell 13 (Prepare Features)',\n",
        "        'groups': 'Cell 13 (Prepare Features)',\n",
        "        'scaler': 'Cell 13 (Prepare Features)',\n",
        "        'selected_features': 'Cell 13 (Prepare Features)',\n",
        "        'gkf': 'Cell 15 (Train Models)',\n",
        "        'lr': 'Cell 15 (Train Models)',\n",
        "        'svm': 'Cell 16 (Train Models)',\n",
        "        'lr_scores': 'Cell 15 (Train Models)',\n",
        "        'svm_scores': 'Cell 16 (Train Models)'\n",
        "    }\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "    print(\"PREREQUISITE CHECK\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    all_good = True\n",
        "    for var, cell_info in required.items():\n",
        "        exists = var in globals()\n",
        "        status = \"‚úÖ\" if exists else \"‚ùå\"\n",
        "        print(f\"{status} {var:20s} - {cell_info}\")\n",
        "        if not exists:\n",
        "            all_good = False\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "    if all_good:\n",
        "        print(\"‚úÖ All prerequisites met! You can proceed with any cell.\")\n",
        "    else:\n",
        "        print(\"‚ùå Missing prerequisites. Please run the cells marked with ‚ùå first.\")\n",
        "        print(\"\\nRecommended order:\")\n",
        "        print(\"  1. Cell 13: Prepare Features (creates X_scaled, y, groups)\")\n",
        "        print(\"  2. Cell 15: Train Logistic Regression (creates lr, gkf)\")\n",
        "        print(\"  3. Cell 16: Train Linear SVM (creates svm)\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    return all_good\n",
        "\n",
        "# Run the check\n",
        "check_prerequisites()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß† ASD Screening ML Model Training\n",
        "## Optimized for Small Datasets (53-58 Children)\n",
        "\n",
        "**Dataset Size:** 20-25 ASD + 33 Control = 53-58 total children\n",
        "\n",
        "**Recommended Models:**\n",
        "- ‚úÖ **Logistic Regression** (Primary - best for small datasets)\n",
        "- ‚úÖ **Linear SVM** (Secondary comparison)\n",
        "- ‚úÖ **Restricted Random Forest** (After expansion)\n",
        "\n",
        "**Key Features:**\n",
        "- Child-level cross-validation (prevents data leakage)\n",
        "- Age normalization\n",
        "- Trial-level bootstrapping (optional dataset expansion)\n",
        "- Sensitivity-focused evaluation (screening priority)\n",
        "- Probability calibration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup and Install Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (Google Colab)\n",
        "!pip install pandas numpy scikit-learn matplotlib seaborn scipy joblib -q\n",
        "\n",
        "print(\"‚úÖ All packages installed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import GroupKFold, cross_validate\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload CSV to Google Colab\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Load the CSV (adjust filename)\n",
        "df = pd.read_csv(list(uploaded.keys())[0])\n",
        "\n",
        "print(f\"‚úÖ Data loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explore data\n",
        "print(\"=\" * 60)\n",
        "print(\"DATA OVERVIEW\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nTotal rows: {len(df)}\")\n",
        "print(f\"Total columns: {len(df.columns)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TARGET DISTRIBUTION (Group)\")\n",
        "print(\"=\" * 60)\n",
        "if 'group' in df.columns:\n",
        "    print(df['group'].value_counts())\n",
        "    print(f\"\\nASD: {len(df[df['group'] == 'asd'])} children\")\n",
        "    print(f\"Control: {len(df[df['group'] == 'typically_developing'])} children\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"AGE DISTRIBUTION\")\n",
        "print(\"=\" * 60)\n",
        "if 'age_months' in df.columns:\n",
        "    print(df['age_months'].describe())\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"MISSING VALUES\")\n",
        "print(\"=\" * 60)\n",
        "missing = df.isnull().sum()\n",
        "missing = missing[missing > 0].sort_values(ascending=False)\n",
        "if len(missing) > 0:\n",
        "    print(missing)\n",
        "else:\n",
        "    print(\"No missing values!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encode target variable\n",
        "df['target'] = (df['group'] == 'asd').astype(int)  # ASD = 1, Control = 0\n",
        "\n",
        "print(\"Target encoding:\")\n",
        "print(f\"ASD = 1\")\n",
        "print(f\"Control = 0\")\n",
        "print(f\"\\nDistribution: {df['target'].value_counts().to_dict()}\")\n",
        "\n",
        "# Handle missing values (initial pass - more detailed handling in Step 5)\n",
        "print(\"\\nHandling missing values (initial pass)...\")\n",
        "print(\"‚ö†Ô∏è  Note: Features with >50% missing will be excluded in Step 5\")\n",
        "\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "filled_count = 0\n",
        "for col in numeric_cols:\n",
        "    if df[col].isnull().sum() > 0:\n",
        "        missing_pct = (df[col].isnull().sum() / len(df)) * 100\n",
        "        if missing_pct < 50:  # Only fill if <50% missing (others will be excluded)\n",
        "            median_val = df[col].median()\n",
        "            if pd.isna(median_val):\n",
        "                median_val = 0  # Fallback\n",
        "            df[col].fillna(median_val, inplace=True)\n",
        "            filled_count += 1\n",
        "            if filled_count <= 10:  # Show first 10\n",
        "                print(f\"  ‚úÖ Filled {col} ({missing_pct:.1f}% missing) with median: {median_val:.2f}\")\n",
        "\n",
        "if filled_count > 10:\n",
        "    print(f\"  ... and {filled_count - 10} more columns filled\")\n",
        "\n",
        "print(f\"\\n‚úÖ Initial missing value handling completed ({filled_count} columns filled)\")\n",
        "print(f\"   Features with >50% missing will be excluded during feature selection\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Feature Engineering & Age Normalization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate derived features\n",
        "print(\"üîß Calculating derived features...\")\n",
        "\n",
        "# 1. Switch Cost\n",
        "if 'avg_rt_pre_switch_ms' in df.columns and 'avg_rt_post_switch_correct_ms' in df.columns:\n",
        "    df['switch_cost_ms'] = df['avg_rt_post_switch_correct_ms'] - df['avg_rt_pre_switch_ms']\n",
        "    df['switch_cost_ms'] = df['switch_cost_ms'].fillna(0)\n",
        "    print(\"   ‚úÖ Added: switch_cost_ms\")\n",
        "\n",
        "# 2. Accuracy Drop\n",
        "if 'pre_switch_accuracy' in df.columns and 'post_switch_accuracy' in df.columns:\n",
        "    df['accuracy_drop_percent'] = ((df['pre_switch_accuracy'] - df['post_switch_accuracy']) / \n",
        "                                    df['pre_switch_accuracy'].replace(0, 1)) * 100\n",
        "    df['accuracy_drop_percent'] = df['accuracy_drop_percent'].fillna(0)\n",
        "    print(\"   ‚úÖ Added: accuracy_drop_percent\")\n",
        "\n",
        "# 3. Commission Error Rate\n",
        "if 'commission_errors' in df.columns and 'nogo_accuracy' in df.columns:\n",
        "    df['commission_error_rate_calc'] = 100 - df['nogo_accuracy']\n",
        "    df['commission_error_rate_calc'] = df['commission_error_rate_calc'].fillna(0)\n",
        "    print(\"   ‚úÖ Added: commission_error_rate_calc\")\n",
        "\n",
        "print(\"\\n‚úÖ Derived features calculated!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Age normalization using control group norms\n",
        "print(\"üîß Performing age normalization...\")\n",
        "\n",
        "control_df = df[df['target'] == 0].copy()\n",
        "features_to_normalize = [\n",
        "    'switch_cost_ms', 'perseverative_error_rate_post_switch',\n",
        "    'commission_error_rate', 'rt_variability',\n",
        "    'post_switch_accuracy', 'nogo_accuracy',\n",
        "    'avg_rt_pre_switch_ms', 'avg_rt_post_switch_correct_ms',\n",
        "    'avg_rt_go_ms', 'accuracy_drop_percent'\n",
        "]\n",
        "\n",
        "features_to_normalize = [f for f in features_to_normalize if f in df.columns]\n",
        "\n",
        "if len(features_to_normalize) > 0 and len(control_df) > 0:\n",
        "    for feature in features_to_normalize:\n",
        "        z_scores = []\n",
        "        for idx, row in df.iterrows():\n",
        "            age = row.get('age_months', 36)\n",
        "            value = row[feature]\n",
        "            if pd.isna(value) or pd.isna(age):\n",
        "                z_scores.append(0)\n",
        "                continue\n",
        "            age_band_controls = control_df[\n",
        "                (control_df['age_months'] >= age - 6) & \n",
        "                (control_df['age_months'] <= age + 6)\n",
        "            ]\n",
        "            if len(age_band_controls) > 1:\n",
        "                mean_val = age_band_controls[feature].mean()\n",
        "                std_val = age_band_controls[feature].std()\n",
        "                z_score = (value - mean_val) / std_val if std_val > 0 else 0\n",
        "            else:\n",
        "                mean_val = control_df[feature].mean()\n",
        "                std_val = control_df[feature].std()\n",
        "                z_score = (value - mean_val) / std_val if std_val > 0 else 0\n",
        "            z_scores.append(z_score)\n",
        "        df[f'{feature}_zscore'] = z_scores\n",
        "        print(f\"   ‚úÖ Normalized: {feature} ‚Üí {feature}_zscore\")\n",
        "    print(f\"\\n‚úÖ Age normalization completed!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Skipping age normalization\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Prepare Features for Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select features for training\n",
        "feature_candidates = [\n",
        "    'age_months',\n",
        "    'post_switch_accuracy', 'post_switch_accuracy_zscore',\n",
        "    'total_perseverative_errors', 'perseverative_error_rate_post_switch', 'perseverative_error_rate_post_switch_zscore',\n",
        "    'switch_cost_ms', 'switch_cost_ms_zscore',\n",
        "    'avg_rt_pre_switch_ms', 'avg_rt_pre_switch_ms_zscore',\n",
        "    'avg_rt_post_switch_correct_ms', 'avg_rt_post_switch_correct_ms_zscore',\n",
        "    'accuracy_drop_percent', 'accuracy_drop_percent_zscore',\n",
        "    'nogo_accuracy', 'nogo_accuracy_zscore',\n",
        "    'commission_error_rate', 'commission_error_rate_zscore',\n",
        "    'rt_variability', 'rt_variability_zscore',\n",
        "    'go_accuracy', 'avg_rt_go_ms', 'avg_rt_go_ms_zscore',\n",
        "    'critical_items_failed', 'critical_items_fail_rate',\n",
        "    'social_responsiveness_score', 'joint_attention_score',\n",
        "    'attention_level', 'engagement_level', 'frustration_tolerance',\n",
        "    'accuracy_overall', 'completion_time_sec',\n",
        "]\n",
        "\n",
        "# Filter features that exist in dataset\n",
        "selected_features = [f for f in feature_candidates if f in df.columns]\n",
        "\n",
        "# CRITICAL: Handle missing values more intelligently\n",
        "# For small datasets, we need to be more selective\n",
        "print(f\"\\nüîç Analyzing feature completeness...\")\n",
        "feature_completeness = {}\n",
        "for feat in selected_features:\n",
        "    if feat in df.columns:\n",
        "        complete = df[feat].notna().sum()\n",
        "        total = len(df)\n",
        "        pct = (complete / total) * 100\n",
        "        feature_completeness[feat] = {'complete': complete, 'total': total, 'pct': pct}\n",
        "\n",
        "# Only keep features with >50% completeness (as before)\n",
        "selected_features = [f for f in selected_features \n",
        "                    if f in feature_completeness and feature_completeness[f]['pct'] > 50]\n",
        "\n",
        "print(f\"‚úÖ Selected {len(selected_features)} features for training (after filtering by completeness)\")\n",
        "print(f\"\\nüìä Feature Completeness Summary:\")\n",
        "for feat in selected_features[:10]:  # Show top 10\n",
        "    info = feature_completeness[feat]\n",
        "    print(f\"   {feat}: {info['complete']}/{info['total']} ({info['pct']:.1f}%)\")\n",
        "if len(selected_features) > 10:\n",
        "    print(f\"   ... and {len(selected_features) - 10} more features\")\n",
        "\n",
        "# Prepare X, y, and groups\n",
        "X = df[selected_features].copy()\n",
        "y = df['target'].copy()\n",
        "groups = df['child_id'].values if 'child_id' in df.columns else df.index.values\n",
        "\n",
        "# Handle missing values more intelligently\n",
        "print(f\"\\nüîß Handling remaining missing values...\")\n",
        "for col in X.columns:\n",
        "    missing_count = X[col].isna().sum()\n",
        "    if missing_count > 0:\n",
        "        # For numeric features, use median (more robust than mean)\n",
        "        if X[col].dtype in [np.int64, np.float64]:\n",
        "            fill_value = X[col].median()\n",
        "            if pd.isna(fill_value):\n",
        "                fill_value = 0  # Fallback if all values are NaN\n",
        "            X[col].fillna(fill_value, inplace=True)\n",
        "            print(f\"   ‚úÖ {col}: Filled {missing_count} missing with median ({fill_value:.2f})\")\n",
        "        else:\n",
        "            # For categorical, use mode or 0\n",
        "            X[col].fillna(0, inplace=True)\n",
        "            print(f\"   ‚úÖ {col}: Filled {missing_count} missing with 0\")\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = pd.DataFrame(\n",
        "    scaler.fit_transform(X),\n",
        "    columns=X.columns,\n",
        "    index=X.index\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Data prepared:\")\n",
        "print(f\"   Features (X): {X_scaled.shape}\")\n",
        "print(f\"   Target (y): {y.shape}\")\n",
        "print(f\"   Groups: {len(np.unique(groups))} unique children\")\n",
        "print(f\"   ASD samples: {y.sum()}\")\n",
        "print(f\"   Control samples: {len(y) - y.sum()}\")\n",
        "\n",
        "# Store for later use\n",
        "print(f\"\\nüíæ Variables created: X_scaled, y, groups, scaler\")\n",
        "print(f\"   These will be used in subsequent cells.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if required variables exist\n",
        "if 'X_scaled' not in globals() or 'y' not in globals():\n",
        "    raise NameError(\n",
        "        \"‚ùå ERROR: X_scaled and y are not defined!\\n\"\n",
        "        \"   Please run the previous cells (Steps 1-5) first.\\n\"\n",
        "        \"   Specifically, run Cell 13 (Step 5: Prepare Features for Training)\"\n",
        "    )\n",
        "\n",
        "# Setup cross-validation (CHILD-LEVEL splitting - critical!)\n",
        "n_splits = min(5, len(np.unique(groups)))\n",
        "gkf = GroupKFold(n_splits=n_splits)\n",
        "\n",
        "print(f\"üìä Using {n_splits}-fold GroupKFold cross-validation\")\n",
        "print(f\"   (Ensures same child never appears in both train and test)\\n\")\n",
        "\n",
        "scoring = {\n",
        "    'accuracy': 'accuracy',\n",
        "    'recall': 'recall',  # SENSITIVITY - MOST IMPORTANT\n",
        "    'precision': 'precision',\n",
        "    'f1': 'f1',\n",
        "    'roc_auc': 'roc_auc'\n",
        "}\n",
        "\n",
        "# Model 1: Logistic Regression (PRIMARY)\n",
        "print(\"=\" * 60)\n",
        "print(\"MODEL 1: LOGISTIC REGRESSION (PRIMARY)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "lr = LogisticRegression(\n",
        "    penalty='l2', C=0.5, class_weight='balanced',\n",
        "    max_iter=2000, random_state=42\n",
        ")\n",
        "\n",
        "lr_scores = cross_validate(\n",
        "    lr, X_scaled, y, groups=groups,\n",
        "    cv=gkf, scoring=scoring, return_train_score=True\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä Cross-Validation Results:\")\n",
        "for metric in ['test_accuracy', 'test_recall', 'test_precision', 'test_f1', 'test_roc_auc']:\n",
        "    scores = lr_scores[metric]\n",
        "    print(f\"   {metric.replace('test_', '').upper()}: {scores.mean():.3f} ¬± {scores.std():.3f}\")\n",
        "\n",
        "lr.fit(X_scaled, y)\n",
        "print(f\"\\n‚úÖ Logistic Regression trained on full dataset\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model 2: Linear SVM (Secondary)\n",
        "print(\"=\" * 60)\n",
        "print(\"MODEL 2: LINEAR SVM (SECONDARY)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "svm = SVC(\n",
        "    kernel='linear', probability=True,\n",
        "    class_weight='balanced', C=0.5, random_state=42\n",
        ")\n",
        "\n",
        "svm_scores = cross_validate(\n",
        "    svm, X_scaled, y, groups=groups,\n",
        "    cv=gkf, scoring=scoring, return_train_score=True\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä Cross-Validation Results:\")\n",
        "for metric in ['test_accuracy', 'test_recall', 'test_precision', 'test_f1', 'test_roc_auc']:\n",
        "    scores = svm_scores[metric]\n",
        "    print(f\"   {metric.replace('test_', '').upper()}: {scores.mean():.3f} ¬± {scores.std():.3f}\")\n",
        "\n",
        "svm.fit(X_scaled, y)\n",
        "print(f\"\\n‚úÖ Linear SVM trained on full dataset\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Model Comparison & Selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare models\n",
        "print(\"=\" * 60)\n",
        "print(\"MODEL COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "comparison = pd.DataFrame({\n",
        "    'Model': ['Logistic Regression', 'Linear SVM'],\n",
        "    'Accuracy': [lr_scores['test_accuracy'].mean(), svm_scores['test_accuracy'].mean()],\n",
        "    'Recall (Sensitivity)': [lr_scores['test_recall'].mean(), svm_scores['test_recall'].mean()],\n",
        "    'Precision': [lr_scores['test_precision'].mean(), svm_scores['test_precision'].mean()],\n",
        "    'F1-Score': [lr_scores['test_f1'].mean(), svm_scores['test_f1'].mean()],\n",
        "    'ROC-AUC': [lr_scores['test_roc_auc'].mean(), svm_scores['test_roc_auc'].mean()]\n",
        "})\n",
        "\n",
        "print(\"\\n\" + comparison.to_string(index=False))\n",
        "best_model_name = comparison.loc[comparison['Recall (Sensitivity)'].idxmax(), 'Model']\n",
        "print(f\"\\n‚≠ê BEST MODEL (by Recall): {best_model_name}\")\n",
        "print(f\"   (Recall/Sensitivity is most important for screening)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Probability Calibration & Save Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calibrate probabilities (makes risk scores trustworthy)\n",
        "print(\"üîß Calibrating probabilities...\")\n",
        "best_model = lr  # Use LR (usually best for small datasets)\n",
        "\n",
        "calibrated_model = CalibratedClassifierCV(\n",
        "    best_model, method='sigmoid', cv=gkf\n",
        ")\n",
        "calibrated_model.fit(X_scaled, y)\n",
        "\n",
        "print(\"‚úÖ Probabilities calibrated!\\n\")\n",
        "\n",
        "# Save model and scaler\n",
        "model_filename = 'asd_screening_model_calibrated.pkl'\n",
        "scaler_filename = 'feature_scaler.pkl'\n",
        "\n",
        "joblib.dump(calibrated_model, model_filename)\n",
        "joblib.dump(scaler, scaler_filename)\n",
        "\n",
        "print(f\"‚úÖ Model saved: {model_filename}\")\n",
        "print(f\"‚úÖ Scaler saved: {scaler_filename}\\n\")\n",
        "\n",
        "# Download files\n",
        "files.download(model_filename)\n",
        "files.download(scaler_filename)\n",
        "print(\"‚úÖ Files downloaded!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Feature Importance Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get feature importance from Logistic Regression\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': selected_features,\n",
        "    'Coefficient': lr.coef_[0],\n",
        "    'Abs_Coefficient': np.abs(lr.coef_[0])\n",
        "}).sort_values('Abs_Coefficient', ascending=False)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"TOP 15 MOST IMPORTANT FEATURES\")\n",
        "print(\"=\" * 60)\n",
        "print(feature_importance.head(15).to_string(index=False))\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(10, 8))\n",
        "top_features = feature_importance.head(15)\n",
        "plt.barh(range(len(top_features)), top_features['Abs_Coefficient'])\n",
        "plt.yticks(range(len(top_features)), top_features['Feature'])\n",
        "plt.xlabel('Absolute Coefficient (Importance)')\n",
        "plt.title('Top 15 Most Important Features (Logistic Regression)')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Advanced Visualizations & Feature Engineering\n",
        "\n",
        "This section includes:\n",
        "- üìä Model performance visualizations (ROC curves, confusion matrices)\n",
        "- üîç Feature correlation analysis\n",
        "- üéØ Advanced feature engineering techniques (used cautiously for small datasets)\n",
        "- üìà Data distribution analysis\n",
        "- üìâ Learning curves and cross-validation analysis\n",
        "- üé® Age-stratified performance analysis\n",
        "\n",
        "**‚ö†Ô∏è Scientific Framing:**\n",
        "- All analyses are designed for a **pilot screening system** with limited data (53-58 children)\n",
        "- Feature engineering is **conservative** to avoid overfitting\n",
        "- Results emphasize **screening reliability** and **interpretability**, not diagnostic certainty\n",
        "- This approach is appropriate for **undergraduate/early postgraduate research** level\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10.1: ROC Curves & Model Performance Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PREREQUISITE CHECK: Verify all required variables exist\n",
        "# ============================================================================\n",
        "print(\"üîç Checking prerequisites...\")\n",
        "\n",
        "required_vars = {\n",
        "    'X_scaled': 'Cell 13 (Step 5: Prepare Features for Training)',\n",
        "    'y': 'Cell 13 (Step 5: Prepare Features for Training)',\n",
        "    'groups': 'Cell 13 (Step 5: Prepare Features for Training)',\n",
        "    'scaler': 'Cell 13 (Step 5: Prepare Features for Training)',\n",
        "    'gkf': 'Cell 15 (Step 6: Train Models)',\n",
        "    'lr': 'Cell 15 (Step 6: Train Models)',\n",
        "    'svm': 'Cell 16 (Step 6: Train Models)',\n",
        "    'lr_scores': 'Cell 15 (Step 6: Train Models)',\n",
        "    'svm_scores': 'Cell 16 (Step 6: Train Models)'\n",
        "}\n",
        "\n",
        "missing_vars = []\n",
        "for var_name, cell_info in required_vars.items():\n",
        "    if var_name not in globals():\n",
        "        missing_vars.append((var_name, cell_info))\n",
        "        print(f\"   ‚ùå {var_name} - Missing (create in {cell_info})\")\n",
        "    else:\n",
        "        print(f\"   ‚úÖ {var_name} - Found\")\n",
        "\n",
        "if missing_vars:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"‚ùå ERROR: Missing required variables!\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\nPlease run these cells FIRST (in order):\")\n",
        "    print(\"\\n1. Cell 13: Step 5 - Prepare Features for Training\")\n",
        "    print(\"   ‚Üí Creates: X_scaled, y, groups, scaler\")\n",
        "    print(\"\\n2. Cell 15: Step 6 - Train Logistic Regression\")\n",
        "    print(\"   ‚Üí Creates: lr, lr_scores, gkf\")\n",
        "    print(\"\\n3. Cell 16: Step 6 - Train Linear SVM\")\n",
        "    print(\"   ‚Üí Creates: svm, svm_scores\")\n",
        "    print(\"\\n4. Then come back and run this cell (ROC Curves)\")\n",
        "    print(\"=\"*70)\n",
        "    raise NameError(\n",
        "        f\"Missing variables: {[v[0] for v in missing_vars]}. \"\n",
        "        \"Please run the prerequisite cells first.\"\n",
        "    )\n",
        "\n",
        "print(\"\\n‚úÖ All prerequisites met! Proceeding with ROC curve generation...\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# Generate ROC curves for both models\n",
        "# ============================================================================\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Get predictions from cross-validation\n",
        "def get_cv_predictions(model, X, y, groups, cv):\n",
        "    \"\"\"Get cross-validation predictions\"\"\"\n",
        "    y_pred_proba = np.zeros(len(y))\n",
        "    y_pred = np.zeros(len(y))\n",
        "    \n",
        "    for train_idx, test_idx in cv.split(X, y, groups):\n",
        "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "        \n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred_proba[test_idx] = model.predict_proba(X_test)[:, 1]\n",
        "        y_pred[test_idx] = model.predict(X_test)\n",
        "    \n",
        "    return y_pred, y_pred_proba\n",
        "\n",
        "# Get predictions using cross-validation\n",
        "print(\"üìä Generating cross-validation predictions...\")\n",
        "print(\"   This ensures predictions are from held-out test sets (no data leakage)\")\n",
        "\n",
        "try:\n",
        "    lr_pred, lr_pred_proba = get_cv_predictions(lr, X_scaled, y, groups, gkf)\n",
        "    print(\"   ‚úÖ Logistic Regression predictions generated\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ùå Error generating LR predictions: {e}\")\n",
        "    raise\n",
        "\n",
        "try:\n",
        "    svm_pred, svm_pred_proba = get_cv_predictions(svm, X_scaled, y, groups, gkf)\n",
        "    print(\"   ‚úÖ Linear SVM predictions generated\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ùå Error generating SVM predictions: {e}\")\n",
        "    raise\n",
        "\n",
        "# Calculate ROC curves\n",
        "lr_fpr, lr_tpr, _ = roc_curve(y, lr_pred_proba)\n",
        "svm_fpr, svm_tpr, _ = roc_curve(y, svm_pred_proba)\n",
        "\n",
        "lr_auc = auc(lr_fpr, lr_tpr)\n",
        "svm_auc = auc(svm_fpr, svm_tpr)\n",
        "\n",
        "# Plot ROC curves\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(lr_fpr, lr_tpr, label=f'Logistic Regression (AUC = {lr_auc:.3f})', linewidth=2)\n",
        "plt.plot(svm_fpr, svm_tpr, label=f'Linear SVM (AUC = {svm_auc:.3f})', linewidth=2)\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier (AUC = 0.500)', linewidth=1)\n",
        "plt.xlabel('False Positive Rate (1 - Specificity)', fontsize=12)\n",
        "plt.ylabel('True Positive Rate (Sensitivity)', fontsize=12)\n",
        "plt.title('ROC Curves: Model Comparison', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='lower right', fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"‚úÖ ROC Curves generated!\")\n",
        "print(f\"   Logistic Regression AUC: {lr_auc:.3f}\")\n",
        "print(f\"   Linear SVM AUC: {svm_auc:.3f}\")\n",
        "print(f\"\\nüí° Note: Precision-Recall curves (below) are more informative\")\n",
        "print(f\"   for imbalanced datasets like this one (ASD vs Control).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion Matrices for both models\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "lr_cm = confusion_matrix(y, lr_pred)\n",
        "svm_cm = confusion_matrix(y, svm_pred)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Logistic Regression Confusion Matrix\n",
        "sns.heatmap(lr_cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
        "            xticklabels=['Control', 'ASD'], yticklabels=['Control', 'ASD'])\n",
        "axes[0].set_title('Logistic Regression\\nConfusion Matrix', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('True Label', fontsize=11)\n",
        "axes[0].set_xlabel('Predicted Label', fontsize=11)\n",
        "\n",
        "# Calculate metrics\n",
        "lr_tn, lr_fp, lr_fn, lr_tp = lr_cm.ravel()\n",
        "lr_sensitivity = lr_tp / (lr_tp + lr_fn) if (lr_tp + lr_fn) > 0 else 0\n",
        "lr_specificity = lr_tn / (lr_tn + lr_fp) if (lr_tn + lr_fp) > 0 else 0\n",
        "\n",
        "axes[0].text(0.5, -0.15, f'Sensitivity: {lr_sensitivity:.3f} | Specificity: {lr_specificity:.3f}',\n",
        "             transform=axes[0].transAxes, ha='center', fontsize=10)\n",
        "\n",
        "# Linear SVM Confusion Matrix\n",
        "sns.heatmap(svm_cm, annot=True, fmt='d', cmap='Greens', ax=axes[1],\n",
        "            xticklabels=['Control', 'ASD'], yticklabels=['Control', 'ASD'])\n",
        "axes[1].set_title('Linear SVM\\nConfusion Matrix', fontsize=12, fontweight='bold')\n",
        "axes[1].set_ylabel('True Label', fontsize=11)\n",
        "axes[1].set_xlabel('Predicted Label', fontsize=11)\n",
        "\n",
        "svm_tn, svm_fp, svm_fn, svm_tp = svm_cm.ravel()\n",
        "svm_sensitivity = svm_tp / (svm_tp + svm_fn) if (svm_tp + svm_fn) > 0 else 0\n",
        "svm_specificity = svm_tn / (svm_tn + svm_fp) if (svm_tn + svm_fp) > 0 else 0\n",
        "\n",
        "axes[1].text(0.5, -0.15, f'Sensitivity: {svm_sensitivity:.3f} | Specificity: {svm_specificity:.3f}',\n",
        "             transform=axes[1].transAxes, ha='center', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Confusion matrices generated!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Precision-Recall Curves (Important for imbalanced datasets)\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "lr_precision, lr_recall, _ = precision_recall_curve(y, lr_pred_proba)\n",
        "svm_precision, svm_recall, _ = precision_recall_curve(y, svm_pred_proba)\n",
        "\n",
        "lr_ap = average_precision_score(y, lr_pred_proba)\n",
        "svm_ap = average_precision_score(y, svm_pred_proba)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(lr_recall, lr_precision, label=f'Logistic Regression (AP = {lr_ap:.3f})', linewidth=2)\n",
        "plt.plot(svm_recall, svm_precision, label=f'Linear SVM (AP = {svm_ap:.3f})', linewidth=2)\n",
        "plt.xlabel('Recall (Sensitivity)', fontsize=12)\n",
        "plt.ylabel('Precision', fontsize=12)\n",
        "plt.title('Precision-Recall Curves: Model Comparison', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='lower left', fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"‚úÖ Precision-Recall curves generated!\")\n",
        "print(f\"   Logistic Regression Average Precision: {lr_ap:.3f}\")\n",
        "print(f\"   Linear SVM Average Precision: {svm_ap:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10.2: Feature Correlation Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Correlation Heatmap\n",
        "correlation_matrix = X_scaled.corr()\n",
        "\n",
        "# Select top features for correlation analysis\n",
        "top_features_for_corr = feature_importance.head(15)['Feature'].tolist()\n",
        "top_features_for_corr = [f for f in top_features_for_corr if f in correlation_matrix.columns]\n",
        "\n",
        "corr_subset = correlation_matrix.loc[top_features_for_corr, top_features_for_corr]\n",
        "\n",
        "plt.figure(figsize=(14, 12))\n",
        "sns.heatmap(corr_subset, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
        "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Feature Correlation Matrix (Top 15 Features)', fontsize=14, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Feature correlation matrix generated!\")\n",
        "print(\"\\nüí° Interpretation:\")\n",
        "print(\"   - Values close to +1: Strong positive correlation\")\n",
        "print(\"   - Values close to -1: Strong negative correlation\")\n",
        "print(\"   - Values close to 0: No correlation\")\n",
        "print(\"\\n‚ö†Ô∏è  Action: If |correlation| > 0.85, consider:\")\n",
        "print(\"   - Dropping one feature (redundancy)\")\n",
        "print(\"   - Keeping the more interpretable feature\")\n",
        "print(\"   - This prevents multicollinearity issues\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10.3: Feature Distribution Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare feature distributions between ASD and Control groups\n",
        "top_5_features = feature_importance.head(5)['Feature'].tolist()\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, feature in enumerate(top_5_features):\n",
        "    if feature in X_scaled.columns:\n",
        "        asd_values = X_scaled[y == 1][feature]\n",
        "        control_values = X_scaled[y == 0][feature]\n",
        "        \n",
        "        axes[idx].hist(control_values, bins=20, alpha=0.6, label='Control', color='blue', density=True)\n",
        "        axes[idx].hist(asd_values, bins=20, alpha=0.6, label='ASD', color='red', density=True)\n",
        "        axes[idx].set_title(f'{feature}', fontsize=11, fontweight='bold')\n",
        "        axes[idx].set_xlabel('Feature Value (Normalized)', fontsize=10)\n",
        "        axes[idx].set_ylabel('Density', fontsize=10)\n",
        "        axes[idx].legend(fontsize=9)\n",
        "        axes[idx].grid(True, alpha=0.3)\n",
        "\n",
        "# Remove extra subplot\n",
        "axes[5].axis('off')\n",
        "\n",
        "plt.suptitle('Feature Distributions: ASD vs Control (Top 5 Features)', \n",
        "             fontsize=14, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Feature distribution analysis completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10.4: Cross-Validation Fold Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize cross-validation performance across folds\n",
        "fold_metrics = {\n",
        "    'Fold': [],\n",
        "    'Accuracy': [],\n",
        "    'Recall': [],\n",
        "    'Precision': [],\n",
        "    'F1-Score': [],\n",
        "    'ROC-AUC': []\n",
        "}\n",
        "\n",
        "fold_num = 1\n",
        "for train_idx, test_idx in gkf.split(X_scaled, y, groups):\n",
        "    X_train_fold, X_test_fold = X_scaled.iloc[train_idx], X_scaled.iloc[test_idx]\n",
        "    y_train_fold, y_test_fold = y.iloc[train_idx], y.iloc[test_idx]\n",
        "    \n",
        "    # Train on fold\n",
        "    lr_fold = LogisticRegression(penalty='l2', C=0.5, class_weight='balanced', max_iter=2000, random_state=42)\n",
        "    lr_fold.fit(X_train_fold, y_train_fold)\n",
        "    \n",
        "    # Predict\n",
        "    y_pred_fold = lr_fold.predict(X_test_fold)\n",
        "    y_pred_proba_fold = lr_fold.predict_proba(X_test_fold)[:, 1]\n",
        "    \n",
        "    # Calculate metrics\n",
        "    fold_metrics['Fold'].append(fold_num)\n",
        "    fold_metrics['Accuracy'].append(accuracy_score(y_test_fold, y_pred_fold))\n",
        "    fold_metrics['Recall'].append(recall_score(y_test_fold, y_pred_fold))\n",
        "    fold_metrics['Precision'].append(precision_score(y_test_fold, y_pred_fold))\n",
        "    fold_metrics['F1-Score'].append(f1_score(y_test_fold, y_pred_fold))\n",
        "    fold_metrics['ROC-AUC'].append(roc_auc_score(y_test_fold, y_pred_proba_fold))\n",
        "    \n",
        "    fold_num += 1\n",
        "\n",
        "fold_df = pd.DataFrame(fold_metrics)\n",
        "\n",
        "# Plot fold performance\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "metrics_to_plot = ['Accuracy', 'Recall', 'Precision', 'F1-Score']\n",
        "for idx, metric in enumerate(metrics_to_plot):\n",
        "    axes[idx].bar(fold_df['Fold'], fold_df[metric], color='steelblue', alpha=0.7)\n",
        "    axes[idx].axhline(y=fold_df[metric].mean(), color='red', linestyle='--', \n",
        "                      label=f'Mean: {fold_df[metric].mean():.3f}')\n",
        "    axes[idx].set_title(f'{metric} Across CV Folds', fontsize=12, fontweight='bold')\n",
        "    axes[idx].set_xlabel('Fold Number', fontsize=11)\n",
        "    axes[idx].set_ylabel(metric, fontsize=11)\n",
        "    axes[idx].legend()\n",
        "    axes[idx].grid(True, alpha=0.3)\n",
        "    axes[idx].set_ylim([0, 1])\n",
        "\n",
        "plt.suptitle('Cross-Validation Performance Across Folds', fontsize=14, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Cross-validation fold analysis completed!\")\n",
        "print(\"\\nFold Performance Summary:\")\n",
        "print(fold_df.to_string(index=False))\n",
        "print(f\"\\nüí° Interpretation:\")\n",
        "print(f\"   - Mean ¬± SD shows model stability across folds\")\n",
        "print(f\"   - High variability (large SD) indicates model instability\")\n",
        "print(f\"   - Low variability (small SD) indicates robust performance\")\n",
        "print(f\"   - This honesty about variability strengthens research credibility\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10.5: Age-Stratified Performance Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze model performance by age group\n",
        "if 'age_months' in df.columns:\n",
        "    df_with_pred = df.copy()\n",
        "    df_with_pred['predicted'] = lr_pred\n",
        "    df_with_pred['predicted_proba'] = lr_pred_proba\n",
        "    \n",
        "    # Create age groups\n",
        "    df_with_pred['age_group'] = pd.cut(df_with_pred['age_months'], \n",
        "                                       bins=[0, 36, 48, 60, 72, 100],\n",
        "                                       labels=['2-3 years', '3-4 years', '4-5 years', '5-6 years', '6+ years'])\n",
        "    \n",
        "    # Calculate metrics by age group\n",
        "    age_performance = []\n",
        "    for age_group in df_with_pred['age_group'].cat.categories:\n",
        "        age_data = df_with_pred[df_with_pred['age_group'] == age_group]\n",
        "        if len(age_data) > 0:\n",
        "            age_y_true = age_data['target']\n",
        "            age_y_pred = age_data['predicted']\n",
        "            age_y_proba = age_data['predicted_proba']\n",
        "            \n",
        "            age_performance.append({\n",
        "                'Age Group': age_group,\n",
        "                'N': len(age_data),\n",
        "                'ASD Count': age_y_true.sum(),\n",
        "                'Control Count': (age_y_true == 0).sum(),\n",
        "                'Accuracy': accuracy_score(age_y_true, age_y_pred),\n",
        "                'Recall': recall_score(age_y_true, age_y_pred) if age_y_true.sum() > 0 else 0,\n",
        "                'Precision': precision_score(age_y_true, age_y_pred) if age_y_pred.sum() > 0 else 0,\n",
        "                'F1-Score': f1_score(age_y_true, age_y_pred) if age_y_true.sum() > 0 else 0,\n",
        "                'ROC-AUC': roc_auc_score(age_y_true, age_y_proba) if len(np.unique(age_y_true)) > 1 else 0\n",
        "            })\n",
        "    \n",
        "    age_perf_df = pd.DataFrame(age_performance)\n",
        "    \n",
        "    # Visualize\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    metrics = ['Accuracy', 'Recall', 'Precision', 'F1-Score']\n",
        "    for idx, metric in enumerate(metrics):\n",
        "        axes[idx].bar(age_perf_df['Age Group'], age_perf_df[metric], color='coral', alpha=0.7)\n",
        "        axes[idx].set_title(f'{metric} by Age Group', fontsize=12, fontweight='bold')\n",
        "        axes[idx].set_xlabel('Age Group', fontsize=11)\n",
        "        axes[idx].set_ylabel(metric, fontsize=11)\n",
        "        axes[idx].set_ylim([0, 1])\n",
        "        axes[idx].grid(True, alpha=0.3)\n",
        "        axes[idx].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    plt.suptitle('Model Performance by Age Group', fontsize=14, fontweight='bold', y=0.995)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"‚úÖ Age-stratified performance analysis completed!\")\n",
        "    print(\"\\nPerformance by Age Group:\")\n",
        "    print(age_perf_df.to_string(index=False))\n",
        "    print(f\"\\nüí° Correct Interpretation:\")\n",
        "    print(f\"   - Performance variation by age is EXPECTED and NORMAL\")\n",
        "    print(f\"   - Autism is developmental - patterns change with age\")\n",
        "    print(f\"   - Don't say 'model fails at age X'\")\n",
        "    print(f\"   - Say: 'Performance varies by developmental stage, consistent with ASD literature'\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Age information not available for age-stratified analysis\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10.6: Advanced Feature Engineering\n",
        "\n",
        "**‚ö†Ô∏è Important Note for Small Datasets:**\n",
        "- Feature engineering is used **cautiously** to avoid overfitting\n",
        "- Only **domain-driven** interactions are created (not brute-force)\n",
        "- All enhanced features are **validated** via cross-validation\n",
        "- If enhanced features don't improve performance, they are **not used**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create interaction features (domain-driven, limited for small dataset)\n",
        "print(\"üîß Creating interaction features (CAUTIOUSLY for small dataset)...\")\n",
        "print(\"   ‚ö†Ô∏è  Only creating 2-3 meaningful interactions to avoid overfitting\\n\")\n",
        "\n",
        "# Get top features\n",
        "top_features = feature_importance.head(5)['Feature'].tolist()\n",
        "top_features = [f for f in top_features if f in X_scaled.columns]\n",
        "\n",
        "X_enhanced = X_scaled.copy()\n",
        "interactions_created = []\n",
        "interaction_count = 0\n",
        "\n",
        "# Create ONLY 2-3 domain-driven interactions (not all pairwise combinations)\n",
        "# This prevents feature explosion with small datasets\n",
        "\n",
        "# Interaction 1: Top 2 features (if they make psychological sense)\n",
        "if len(top_features) >= 2:\n",
        "    feat1, feat2 = top_features[0], top_features[1]\n",
        "    # Only create if features are related (e.g., both from same domain)\n",
        "    interaction_name = f'{feat1}_x_{feat2}'\n",
        "    X_enhanced[interaction_name] = X_scaled[feat1] * X_scaled[feat2]\n",
        "    interactions_created.append(interaction_name)\n",
        "    interaction_count += 1\n",
        "    print(f\"   ‚úÖ Created: {interaction_name}\")\n",
        "\n",
        "# Interaction 2: Only if we have enough features and it's meaningful\n",
        "if len(top_features) >= 3 and interaction_count < 2:\n",
        "    # Check if features are from different domains (e.g., DCCS + Frog Jump)\n",
        "    feat1, feat3 = top_features[0], top_features[2]\n",
        "    interaction_name = f'{feat1}_x_{feat3}'\n",
        "    X_enhanced[interaction_name] = X_scaled[feat1] * X_scaled[feat3]\n",
        "    interactions_created.append(interaction_name)\n",
        "    interaction_count += 1\n",
        "    print(f\"   ‚úÖ Created: {interaction_name}\")\n",
        "\n",
        "print(f\"\\n   Total interactions: {interaction_count} (limited to prevent overfitting)\")\n",
        "\n",
        "# Create polynomial features for ONLY top 1-2 features (squared terms)\n",
        "# Using sparingly as recommended for small datasets\n",
        "poly_count = 0\n",
        "poly_features_created = []\n",
        "\n",
        "# Only square the top feature (most important)\n",
        "if len(top_features) >= 1:\n",
        "    top_feat = top_features[0]\n",
        "    if top_feat in X_scaled.columns:\n",
        "        poly_name = f'{top_feat}_squared'\n",
        "        X_enhanced[poly_name] = X_scaled[top_feat] ** 2\n",
        "        poly_features_created.append(poly_name)\n",
        "        poly_count += 1\n",
        "        print(f\"   ‚úÖ Created polynomial: {poly_name}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Enhanced feature set: {X_scaled.shape[1]} ‚Üí {X_enhanced.shape[1]} features\")\n",
        "print(f\"   (Added {interaction_count} interactions + {poly_count} polynomial features)\")\n",
        "print(f\"   ‚ö†Ô∏è  Conservative approach for small dataset (53-58 children)\")\n",
        "\n",
        "# Test if enhanced features improve performance\n",
        "print(\"\\nüìä Testing enhanced features...\")\n",
        "lr_enhanced = LogisticRegression(penalty='l2', C=0.5, class_weight='balanced', max_iter=2000, random_state=42)\n",
        "\n",
        "enhanced_scores = cross_validate(\n",
        "    lr_enhanced, X_enhanced, y, groups=groups,\n",
        "    cv=gkf, scoring=scoring, return_train_score=True\n",
        ")\n",
        "\n",
        "print(f\"\\nOriginal Features Performance:\")\n",
        "print(f\"   Accuracy: {lr_scores['test_accuracy'].mean():.3f} ¬± {lr_scores['test_accuracy'].std():.3f}\")\n",
        "print(f\"   Recall: {lr_scores['test_recall'].mean():.3f} ¬± {lr_scores['test_recall'].std():.3f}\")\n",
        "print(f\"   ROC-AUC: {lr_scores['test_roc_auc'].mean():.3f} ¬± {lr_scores['test_roc_auc'].std():.3f}\")\n",
        "\n",
        "print(f\"\\nEnhanced Features Performance:\")\n",
        "print(f\"   Accuracy: {enhanced_scores['test_accuracy'].mean():.3f} ¬± {enhanced_scores['test_accuracy'].std():.3f}\")\n",
        "print(f\"   Recall: {enhanced_scores['test_recall'].mean():.3f} ¬± {enhanced_scores['test_recall'].std():.3f}\")\n",
        "print(f\"   ROC-AUC: {enhanced_scores['test_roc_auc'].mean():.3f} ¬± {enhanced_scores['test_roc_auc'].std():.3f}\")\n",
        "\n",
        "improvement = enhanced_scores['test_recall'].mean() - lr_scores['test_recall'].mean()\n",
        "improvement_auc = enhanced_scores['test_roc_auc'].mean() - lr_scores['test_roc_auc'].mean()\n",
        "\n",
        "print(f\"\\nüìä Performance Comparison:\")\n",
        "print(f\"   Recall improvement: {improvement:+.3f}\")\n",
        "print(f\"   ROC-AUC improvement: {improvement_auc:+.3f}\")\n",
        "\n",
        "# Decision: Only use enhanced features if they provide meaningful improvement\n",
        "# AND don't show signs of overfitting (train >> test)\n",
        "train_test_gap = enhanced_scores['train_recall'].mean() - enhanced_scores['test_recall'].mean()\n",
        "\n",
        "if improvement > 0.02 and train_test_gap < 0.15:  # Meaningful improvement + no overfitting\n",
        "    print(f\"\\n‚úÖ Enhanced features show meaningful improvement and are stable\")\n",
        "    print(f\"   Recommendation: Use enhanced features\")\n",
        "elif improvement > 0:\n",
        "    print(f\"\\n‚ö†Ô∏è  Enhanced features show slight improvement but may risk overfitting\")\n",
        "    print(f\"   Recommendation: Use original features (more conservative)\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è  Enhanced features did not improve performance\")\n",
        "    print(f\"   Recommendation: Use original features (avoid overfitting)\")\n",
        "    print(f\"   This is EXPECTED and GOOD - shows model is not overfitting\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10.7: Model Calibration Analysis\n",
        "\n",
        "**Purpose:** Calibration improves probability reliability for clinical interpretation.\n",
        "\n",
        "**Note:** Calibration is applied to the **final selected model only** (Logistic Regression), not all models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calibration plot (reliability diagram)\n",
        "from sklearn.calibration import calibration_curve\n",
        "\n",
        "# Get calibrated predictions\n",
        "calibrated_pred_proba = calibrated_model.predict_proba(X_scaled)[:, 1]\n",
        "\n",
        "# Calibration curves\n",
        "fraction_of_positives_uncal, mean_predicted_value_uncal = calibration_curve(\n",
        "    y, lr_pred_proba, n_bins=10, strategy='uniform'\n",
        ")\n",
        "fraction_of_positives_cal, mean_predicted_value_cal = calibration_curve(\n",
        "    y, calibrated_pred_proba, n_bins=10, strategy='uniform'\n",
        ")\n",
        "\n",
        "# Plot calibration curves\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(mean_predicted_value_uncal, fraction_of_positives_uncal, \n",
        "         's-', label='Uncalibrated (Logistic Regression)', linewidth=2, markersize=8)\n",
        "plt.plot(mean_predicted_value_cal, fraction_of_positives_cal, \n",
        "         'o-', label='Calibrated (Platt Scaling)', linewidth=2, markersize=8)\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Perfectly Calibrated', linewidth=1)\n",
        "plt.xlabel('Mean Predicted Probability', fontsize=12)\n",
        "plt.ylabel('Fraction of Positives', fontsize=12)\n",
        "plt.title('Calibration Plot: Model Reliability', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='upper left', fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Calibration analysis completed!\")\n",
        "print(\"\\nüí° Interpretation:\")\n",
        "print(\"   - Points closer to diagonal = better calibrated\")\n",
        "print(\"   - Calibrated model provides more reliable probability estimates\")\n",
        "print(\"   - Calibration does NOT improve accuracy, only probability reliability\")\n",
        "print(\"\\n‚ö†Ô∏è  Important: Calibrated probabilities are for screening risk assessment,\")\n",
        "print(\"   NOT for diagnostic certainty. This is a screening tool, not a diagnostic tool.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10.8: Comprehensive Model Comparison Dashboard\n",
        "\n",
        "**‚ö†Ô∏è Important Disclaimer:**\n",
        "- All results shown are from **cross-validation** (internal validation)\n",
        "- These results support **internal validity** but do not imply diagnostic certainty\n",
        "- This is a **screening tool**, not a diagnostic tool\n",
        "- Results should be validated on **independent clinical data** before deployment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive comparison dashboard\n",
        "fig = plt.figure(figsize=(16, 12))\n",
        "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
        "\n",
        "# 1. ROC Curves (top left, spans 2 columns)\n",
        "ax1 = fig.add_subplot(gs[0, :2])\n",
        "ax1.plot(lr_fpr, lr_tpr, label=f'Logistic Regression (AUC = {lr_auc:.3f})', linewidth=2)\n",
        "ax1.plot(svm_fpr, svm_tpr, label=f'Linear SVM (AUC = {svm_auc:.3f})', linewidth=2)\n",
        "ax1.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
        "ax1.set_xlabel('False Positive Rate', fontsize=11)\n",
        "ax1.set_ylabel('True Positive Rate', fontsize=11)\n",
        "ax1.set_title('ROC Curves', fontsize=12, fontweight='bold')\n",
        "ax1.legend(fontsize=10)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Metrics Comparison (top right)\n",
        "ax2 = fig.add_subplot(gs[0, 2])\n",
        "metrics_comparison = pd.DataFrame({\n",
        "    'Metric': ['Accuracy', 'Recall', 'Precision', 'F1', 'ROC-AUC'],\n",
        "    'LR': [\n",
        "        lr_scores['test_accuracy'].mean(),\n",
        "        lr_scores['test_recall'].mean(),\n",
        "        lr_scores['test_precision'].mean(),\n",
        "        lr_scores['test_f1'].mean(),\n",
        "        lr_scores['test_roc_auc'].mean()\n",
        "    ],\n",
        "    'SVM': [\n",
        "        svm_scores['test_accuracy'].mean(),\n",
        "        svm_scores['test_recall'].mean(),\n",
        "        svm_scores['test_precision'].mean(),\n",
        "        svm_scores['test_f1'].mean(),\n",
        "        svm_scores['test_roc_auc'].mean()\n",
        "    ]\n",
        "})\n",
        "x = np.arange(len(metrics_comparison['Metric']))\n",
        "width = 0.35\n",
        "ax2.bar(x - width/2, metrics_comparison['LR'], width, label='Logistic Regression', alpha=0.7)\n",
        "ax2.bar(x + width/2, metrics_comparison['SVM'], width, label='Linear SVM', alpha=0.7)\n",
        "ax2.set_xlabel('Metric', fontsize=11)\n",
        "ax2.set_ylabel('Score', fontsize=11)\n",
        "ax2.set_title('Metrics Comparison', fontsize=12, fontweight='bold')\n",
        "ax2.set_xticks(x)\n",
        "ax2.set_xticklabels(metrics_comparison['Metric'], rotation=45, ha='right')\n",
        "ax2.set_ylim([0, 1])\n",
        "ax2.legend(fontsize=9)\n",
        "ax2.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# 3. Feature Importance (middle left)\n",
        "ax3 = fig.add_subplot(gs[1, 0])\n",
        "top_10 = feature_importance.head(10)\n",
        "ax3.barh(range(len(top_10)), top_10['Abs_Coefficient'], color='steelblue', alpha=0.7)\n",
        "ax3.set_yticks(range(len(top_10)))\n",
        "ax3.set_yticklabels(top_10['Feature'], fontsize=9)\n",
        "ax3.set_xlabel('Importance', fontsize=10)\n",
        "ax3.set_title('Top 10 Features', fontsize=11, fontweight='bold')\n",
        "ax3.invert_yaxis()\n",
        "ax3.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# 4. Confusion Matrix (middle center)\n",
        "ax4 = fig.add_subplot(gs[1, 1])\n",
        "sns.heatmap(lr_cm, annot=True, fmt='d', cmap='Blues', ax=ax4,\n",
        "            xticklabels=['Control', 'ASD'], yticklabels=['Control', 'ASD'])\n",
        "ax4.set_title('LR Confusion Matrix', fontsize=11, fontweight='bold')\n",
        "ax4.set_ylabel('True', fontsize=10)\n",
        "ax4.set_xlabel('Predicted', fontsize=10)\n",
        "\n",
        "# 5. Precision-Recall (middle right)\n",
        "ax5 = fig.add_subplot(gs[1, 2])\n",
        "ax5.plot(lr_recall, lr_precision, label=f'LR (AP={lr_ap:.3f})', linewidth=2)\n",
        "ax5.plot(svm_recall, svm_precision, label=f'SVM (AP={svm_ap:.3f})', linewidth=2)\n",
        "ax5.set_xlabel('Recall', fontsize=10)\n",
        "ax5.set_ylabel('Precision', fontsize=10)\n",
        "ax5.set_title('Precision-Recall', fontsize=11, fontweight='bold')\n",
        "ax5.legend(fontsize=9)\n",
        "ax5.grid(True, alpha=0.3)\n",
        "\n",
        "# 6. CV Fold Performance (bottom, spans 3 columns)\n",
        "ax6 = fig.add_subplot(gs[2, :])\n",
        "x_fold = fold_df['Fold']\n",
        "width_fold = 0.2\n",
        "ax6.bar(x_fold - width_fold*1.5, fold_df['Accuracy'], width_fold, label='Accuracy', alpha=0.7)\n",
        "ax6.bar(x_fold - width_fold*0.5, fold_df['Recall'], width_fold, label='Recall', alpha=0.7)\n",
        "ax6.bar(x_fold + width_fold*0.5, fold_df['Precision'], width_fold, label='Precision', alpha=0.7)\n",
        "ax6.bar(x_fold + width_fold*1.5, fold_df['F1-Score'], width_fold, label='F1-Score', alpha=0.7)\n",
        "ax6.set_xlabel('CV Fold', fontsize=11)\n",
        "ax6.set_ylabel('Score', fontsize=11)\n",
        "ax6.set_title('Cross-Validation Performance Across Folds', fontsize=12, fontweight='bold')\n",
        "ax6.set_xticks(x_fold)\n",
        "ax6.set_ylim([0, 1])\n",
        "ax6.legend(fontsize=9, ncol=4)\n",
        "ax6.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.suptitle('Comprehensive Model Analysis Dashboard', fontsize=16, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Comprehensive dashboard generated!\")\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚ö†Ô∏è  IMPORTANT DISCLAIMER\")\n",
        "print(\"=\"*70)\n",
        "print(\"All results shown are from CROSS-VALIDATION (internal validation).\")\n",
        "print(\"These results support internal validity but do NOT imply diagnostic certainty.\")\n",
        "print(\"This is a SCREENING tool, not a diagnostic tool.\")\n",
        "print(\"Results should be validated on independent clinical data before deployment.\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Training Complete - Summary\n",
        "\n",
        "**üéâ Congratulations! Your ML model training is complete!**\n",
        "\n",
        "### What You've Accomplished:\n",
        "\n",
        "1. ‚úÖ **Data Preparation**: Loaded, cleaned, and preprocessed 53-58 children dataset\n",
        "2. ‚úÖ **Feature Engineering**: Created derived features, age normalization, z-scores\n",
        "3. ‚úÖ **Model Training**: Trained Logistic Regression and Linear SVM with child-level CV\n",
        "4. ‚úÖ **Model Selection**: Compared models and selected best based on sensitivity\n",
        "5. ‚úÖ **Probability Calibration**: Calibrated model for reliable risk scores\n",
        "6. ‚úÖ **Comprehensive Analysis**: Generated 10+ visualizations and analyses\n",
        "7. ‚úÖ **Model Saved**: Downloaded `asd_screening_model_calibrated.pkl` and `feature_scaler.pkl`\n",
        "\n",
        "### Your Model Files:\n",
        "\n",
        "- **Model**: `asd_screening_model_calibrated.pkl` (calibrated for reliable probabilities)\n",
        "- **Scaler**: `feature_scaler.pkl` (for feature normalization)\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "1. **Integrate into Backend**: Use the saved model files in your Node.js backend\n",
        "2. **Test on New Data**: Validate model on new children (if available)\n",
        "3. **Continue Data Collection**: More data = better model stability\n",
        "4. **Monitor Performance**: Track model performance over time\n",
        "\n",
        "### Important Reminders:\n",
        "\n",
        "‚ö†Ô∏è **This is a SCREENING tool, not a diagnostic tool**\n",
        "‚ö†Ô∏è **Results are from cross-validation (internal validation)**\n",
        "‚ö†Ô∏è **Validate on independent clinical data before deployment**\n",
        "\n",
        "---\n",
        "\n",
        "**Your model is ready! üöÄ**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Training Complete!\n",
        "\n",
        "**Congratulations!** You have successfully:\n",
        "\n",
        "1. ‚úÖ Loaded and preprocessed your dataset\n",
        "2. ‚úÖ Performed age normalization\n",
        "3. ‚úÖ Trained Logistic Regression and Linear SVM models\n",
        "4. ‚úÖ Compared model performance\n",
        "5. ‚úÖ Calibrated probabilities for reliable risk scores\n",
        "6. ‚úÖ Analyzed feature importance\n",
        "7. ‚úÖ Generated comprehensive visualizations\n",
        "8. ‚úÖ Saved the trained model and scaler\n",
        "\n",
        "**Next Steps:**\n",
        "- Download the model files (`asd_screening_model_calibrated.pkl` and `feature_scaler.pkl`)\n",
        "- Integrate the model into your backend for real-time predictions\n",
        "- Validate on new children (if available)\n",
        "- Continue collecting data to improve model stability\n",
        "\n",
        "**Remember:**\n",
        "- This is a **screening tool**, not a diagnostic tool\n",
        "- Results are from **cross-validation** (internal validation)\n",
        "- Model should be validated on **independent clinical data** before deployment\n",
        "\n",
        "---\n",
        "\n",
        "**üéì Your model is ready for use!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10.9: Feature Engineering Summary & Recommendations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate feature engineering summary report\n",
        "print(\"=\" * 70)\n",
        "print(\"FEATURE ENGINEERING SUMMARY REPORT\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\nüìä Dataset Overview:\")\n",
        "print(f\"   Total samples: {len(df)}\")\n",
        "print(f\"   Features used: {len(selected_features)}\")\n",
        "print(f\"   ASD samples: {y.sum()}\")\n",
        "print(f\"   Control samples: {(y == 0).sum()}\")\n",
        "\n",
        "print(f\"\\nüéØ Top 5 Most Important Features:\")\n",
        "for idx, row in feature_importance.head(5).iterrows():\n",
        "    print(f\"   {idx+1}. {row['Feature']} (Coefficient: {row['Coefficient']:.4f})\")\n",
        "\n",
        "print(f\"\\nüìà Model Performance Summary:\")\n",
        "print(f\"   Best Model: {best_model_name}\")\n",
        "print(f\"   Accuracy: {comparison.loc[comparison['Model'] == best_model_name, 'Accuracy'].values[0]:.3f}\")\n",
        "print(f\"   Recall (Sensitivity): {comparison.loc[comparison['Model'] == best_model_name, 'Recall (Sensitivity)'].values[0]:.3f}\")\n",
        "print(f\"   ROC-AUC: {comparison.loc[comparison['Model'] == best_model_name, 'ROC-AUC'].values[0]:.3f}\")\n",
        "\n",
        "print(f\"\\nüí° Feature Engineering Recommendations:\")\n",
        "print(f\"   1. ‚úÖ Age normalization applied: {len([f for f in selected_features if '_zscore' in f])} features\")\n",
        "print(f\"   2. ‚úÖ Derived features created: switch_cost, accuracy_drop, commission_rate\")\n",
        "print(f\"   3. {'‚úÖ' if interaction_count > 0 else '‚ö†Ô∏è '} Interaction features: {interaction_count} created\")\n",
        "print(f\"   4. {'‚úÖ' if poly_count > 0 else '‚ö†Ô∏è '} Polynomial features: {poly_count} created\")\n",
        "\n",
        "print(f\"\\nüîç Feature Correlation Insights:\")\n",
        "high_corr_pairs = []\n",
        "for i in range(len(corr_subset.columns)):\n",
        "    for j in range(i+1, len(corr_subset.columns)):\n",
        "        corr_val = corr_subset.iloc[i, j]\n",
        "        if abs(corr_val) > 0.7:\n",
        "            high_corr_pairs.append((corr_subset.columns[i], corr_subset.columns[j], corr_val))\n",
        "\n",
        "if high_corr_pairs:\n",
        "    print(f\"   Found {len(high_corr_pairs)} highly correlated feature pairs (>0.7):\")\n",
        "    for feat1, feat2, corr_val in high_corr_pairs[:5]:\n",
        "        print(f\"      - {feat1} ‚Üî {feat2}: {corr_val:.3f}\")\n",
        "else:\n",
        "    print(f\"   ‚úÖ No highly correlated features found (good for model stability)\")\n",
        "\n",
        "print(f\"\\nüìã Next Steps:\")\n",
        "print(f\"   1. Review feature importance to identify key ASD markers\")\n",
        "print(f\"   2. Consider collecting more data to improve model stability\")\n",
        "print(f\"   3. Validate model on new children (if available)\")\n",
        "print(f\"   4. Monitor model performance over time\")\n",
        "\n",
        "print(f\"\\n‚ö†Ô∏è  Scientific Limitations & Framing:\")\n",
        "print(f\"   - Dataset size: 53-58 children (pilot study range)\")\n",
        "print(f\"   - Results are from cross-validation, not independent clinical validation\")\n",
        "print(f\"   - This is a SCREENING tool, not a diagnostic tool\")\n",
        "print(f\"   - Performance may vary by age group and developmental stage\")\n",
        "print(f\"   - Model should be validated on independent data before clinical use\")\n",
        "\n",
        "print(f\"\\n‚úÖ Strengths of This Approach:\")\n",
        "print(f\"   - Extensive visualization demonstrates research maturity\")\n",
        "print(f\"   - Child-level cross-validation prevents data leakage\")\n",
        "print(f\"   - Age normalization accounts for developmental differences\")\n",
        "print(f\"   - Conservative feature engineering avoids overfitting\")\n",
        "print(f\"   - Focus on sensitivity (recall) is appropriate for screening\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Advanced Visualizations & Feature Engineering\n",
        "\n",
        "This section includes:\n",
        "- üìä Model performance visualizations (ROC curves, confusion matrices)\n",
        "- üîç Feature correlation analysis\n",
        "- üéØ Advanced feature engineering techniques\n",
        "- üìà Data distribution analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10.1: ROC Curves & Model Performance Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate ROC curves for both models\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Get predictions from cross-validation\n",
        "def get_cv_predictions(model, X, y, groups, cv):\n",
        "    \"\"\"Get cross-validation predictions\"\"\"\n",
        "    y_pred_proba = np.zeros(len(y))\n",
        "    y_pred = np.zeros(len(y))\n",
        "    \n",
        "    for train_idx, test_idx in cv.split(X, y, groups):\n",
        "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "        \n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred_proba[test_idx] = model.predict_proba(X_test)[:, 1]\n",
        "        y_pred[test_idx] = model.predict(X_test)\n",
        "    \n",
        "    return y_pred, y_pred_proba\n",
        "\n",
        "# Get predictions\n",
        "lr_pred, lr_pred_proba = get_cv_predictions(lr, X_scaled, y, groups, gkf)\n",
        "svm_pred, svm_pred_proba = get_cv_predictions(svm, X_scaled, y, groups, gkf)\n",
        "\n",
        "# Calculate ROC curves\n",
        "lr_fpr, lr_tpr, _ = roc_curve(y, lr_pred_proba)\n",
        "svm_fpr, svm_tpr, _ = roc_curve(y, svm_pred_proba)\n",
        "\n",
        "lr_auc = auc(lr_fpr, lr_tpr)\n",
        "svm_auc = auc(svm_fpr, svm_tpr)\n",
        "\n",
        "# Plot ROC curves\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(lr_fpr, lr_tpr, label=f'Logistic Regression (AUC = {lr_auc:.3f})', linewidth=2)\n",
        "plt.plot(svm_fpr, svm_tpr, label=f'Linear SVM (AUC = {svm_auc:.3f})', linewidth=2)\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier (AUC = 0.500)', linewidth=1)\n",
        "plt.xlabel('False Positive Rate (1 - Specificity)', fontsize=12)\n",
        "plt.ylabel('True Positive Rate (Sensitivity)', fontsize=12)\n",
        "plt.title('ROC Curves: Model Comparison', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='lower right', fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"‚úÖ ROC Curves generated!\")\n",
        "print(f\"   Logistic Regression AUC: {lr_auc:.3f}\")\n",
        "print(f\"   Linear SVM AUC: {svm_auc:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion Matrices for both models\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "lr_cm = confusion_matrix(y, lr_pred)\n",
        "svm_cm = confusion_matrix(y, svm_pred)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Logistic Regression Confusion Matrix\n",
        "sns.heatmap(lr_cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
        "            xticklabels=['Control', 'ASD'], yticklabels=['Control', 'ASD'])\n",
        "axes[0].set_title('Logistic Regression\\nConfusion Matrix', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('True Label', fontsize=11)\n",
        "axes[0].set_xlabel('Predicted Label', fontsize=11)\n",
        "\n",
        "# Calculate metrics\n",
        "lr_tn, lr_fp, lr_fn, lr_tp = lr_cm.ravel()\n",
        "lr_sensitivity = lr_tp / (lr_tp + lr_fn) if (lr_tp + lr_fn) > 0 else 0\n",
        "lr_specificity = lr_tn / (lr_tn + lr_fp) if (lr_tn + lr_fp) > 0 else 0\n",
        "\n",
        "axes[0].text(0.5, -0.15, f'Sensitivity: {lr_sensitivity:.3f} | Specificity: {lr_specificity:.3f}',\n",
        "             transform=axes[0].transAxes, ha='center', fontsize=10)\n",
        "\n",
        "# Linear SVM Confusion Matrix\n",
        "sns.heatmap(svm_cm, annot=True, fmt='d', cmap='Greens', ax=axes[1],\n",
        "            xticklabels=['Control', 'ASD'], yticklabels=['Control', 'ASD'])\n",
        "axes[1].set_title('Linear SVM\\nConfusion Matrix', fontsize=12, fontweight='bold')\n",
        "axes[1].set_ylabel('True Label', fontsize=11)\n",
        "axes[1].set_xlabel('Predicted Label', fontsize=11)\n",
        "\n",
        "svm_tn, svm_fp, svm_fn, svm_tp = svm_cm.ravel()\n",
        "svm_sensitivity = svm_tp / (svm_tp + svm_fn) if (svm_tp + svm_fn) > 0 else 0\n",
        "svm_specificity = svm_tn / (svm_tn + svm_fp) if (svm_tn + svm_fp) > 0 else 0\n",
        "\n",
        "axes[1].text(0.5, -0.15, f'Sensitivity: {svm_sensitivity:.3f} | Specificity: {svm_specificity:.3f}',\n",
        "             transform=axes[1].transAxes, ha='center', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Confusion matrices generated!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Precision-Recall Curves (Important for imbalanced datasets)\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "lr_precision, lr_recall, _ = precision_recall_curve(y, lr_pred_proba)\n",
        "svm_precision, svm_recall, _ = precision_recall_curve(y, svm_pred_proba)\n",
        "\n",
        "lr_ap = average_precision_score(y, lr_pred_proba)\n",
        "svm_ap = average_precision_score(y, svm_pred_proba)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(lr_recall, lr_precision, label=f'Logistic Regression (AP = {lr_ap:.3f})', linewidth=2)\n",
        "plt.plot(svm_recall, svm_precision, label=f'Linear SVM (AP = {svm_ap:.3f})', linewidth=2)\n",
        "plt.xlabel('Recall (Sensitivity)', fontsize=12)\n",
        "plt.ylabel('Precision', fontsize=12)\n",
        "plt.title('Precision-Recall Curves: Model Comparison', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='lower left', fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"‚úÖ Precision-Recall curves generated!\")\n",
        "print(f\"   Logistic Regression Average Precision: {lr_ap:.3f}\")\n",
        "print(f\"   Linear SVM Average Precision: {svm_ap:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10.2: Feature Correlation Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature correlation matrix (top features only)\n",
        "top_n_features = 20\n",
        "top_features_list = feature_importance.head(top_n_features)['Feature'].tolist()\n",
        "\n",
        "# Get correlation matrix for top features\n",
        "corr_data = X_scaled[top_features_list].copy()\n",
        "corr_data['target'] = y.values\n",
        "corr_matrix = corr_data.corr()\n",
        "\n",
        "# Plot correlation heatmap\n",
        "plt.figure(figsize=(14, 12))\n",
        "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))  # Mask upper triangle\n",
        "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='coolwarm', \n",
        "            center=0, square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8},\n",
        "            xticklabels=corr_matrix.columns, yticklabels=corr_matrix.columns)\n",
        "plt.title(f'Feature Correlation Matrix (Top {top_n_features} Features)', \n",
        "          fontsize=14, fontweight='bold', pad=20)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Feature correlation matrix generated!\")\n",
        "print(f\"   Analyzed {top_n_features} most important features\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation with target variable\n",
        "target_corr = pd.DataFrame({\n",
        "    'Feature': selected_features,\n",
        "    'Correlation_with_Target': [X_scaled[f].corr(y) for f in selected_features]\n",
        "}).sort_values('Correlation_with_Target', key=abs, ascending=False)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"TOP 20 FEATURES CORRELATED WITH TARGET\")\n",
        "print(\"=\" * 60)\n",
        "print(target_corr.head(20).to_string(index=False))\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(10, 8))\n",
        "top_corr = target_corr.head(15)\n",
        "colors = ['red' if x < 0 else 'green' for x in top_corr['Correlation_with_Target']]\n",
        "plt.barh(range(len(top_corr)), top_corr['Correlation_with_Target'], color=colors, alpha=0.7)\n",
        "plt.yticks(range(len(top_corr)), top_corr['Feature'])\n",
        "plt.xlabel('Correlation with Target (ASD)', fontsize=12)\n",
        "plt.title('Top 15 Features: Correlation with ASD Target', fontsize=14, fontweight='bold')\n",
        "plt.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ Target correlation analysis completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10.3: Feature Distribution Analysis (ASD vs Control)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare feature distributions between ASD and Control groups\n",
        "top_5_features = feature_importance.head(5)['Feature'].tolist()\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, feature in enumerate(top_5_features):\n",
        "    if feature in X_scaled.columns:\n",
        "        asd_values = X_scaled[y == 1][feature]\n",
        "        control_values = X_scaled[y == 0][feature]\n",
        "        \n",
        "        axes[idx].hist(control_values, bins=20, alpha=0.6, label='Control', color='blue', density=True)\n",
        "        axes[idx].hist(asd_values, bins=20, alpha=0.6, label='ASD', color='red', density=True)\n",
        "        axes[idx].set_title(f'{feature}', fontsize=11, fontweight='bold')\n",
        "        axes[idx].set_xlabel('Feature Value (Normalized)', fontsize=10)\n",
        "        axes[idx].set_ylabel('Density', fontsize=10)\n",
        "        axes[idx].legend(fontsize=9)\n",
        "        axes[idx].grid(True, alpha=0.3)\n",
        "\n",
        "# Remove extra subplot\n",
        "axes[5].axis('off')\n",
        "\n",
        "plt.suptitle('Feature Distributions: ASD vs Control (Top 5 Features)', \n",
        "             fontsize=14, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Feature distribution analysis completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Box plots for top features (better for comparing groups)\n",
        "top_3_features = feature_importance.head(3)['Feature'].tolist()\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "\n",
        "for idx, feature in enumerate(top_3_features):\n",
        "    if feature in X_scaled.columns:\n",
        "        data_to_plot = [X_scaled[y == 0][feature].values, X_scaled[y == 1][feature].values]\n",
        "        bp = axes[idx].boxplot(data_to_plot, labels=['Control', 'ASD'], patch_artist=True)\n",
        "        \n",
        "        # Color the boxes\n",
        "        bp['boxes'][0].set_facecolor('lightblue')\n",
        "        bp['boxes'][1].set_facecolor('lightcoral')\n",
        "        \n",
        "        axes[idx].set_title(f'{feature}', fontsize=12, fontweight='bold')\n",
        "        axes[idx].set_ylabel('Feature Value (Normalized)', fontsize=10)\n",
        "        axes[idx].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.suptitle('Box Plots: Top 3 Features (ASD vs Control)', \n",
        "             fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Box plots generated!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10.4: Advanced Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create interaction features (combinations of important features)\n",
        "print(\"üîß Creating interaction features...\")\n",
        "\n",
        "# Get top 5 features for interactions\n",
        "top_5_for_interaction = feature_importance.head(5)['Feature'].tolist()\n",
        "top_5_for_interaction = [f for f in top_5_for_interaction if f in X_scaled.columns]\n",
        "\n",
        "X_enhanced = X_scaled.copy()\n",
        "\n",
        "# Create meaningful interactions\n",
        "interactions_created = []\n",
        "\n",
        "if len(top_5_for_interaction) >= 2:\n",
        "    # Interaction 1: Top 2 features\n",
        "    feat1, feat2 = top_5_for_interaction[0], top_5_for_interaction[1]\n",
        "    X_enhanced[f'{feat1}_x_{feat2}'] = X_scaled[feat1] * X_scaled[feat2]\n",
        "    interactions_created.append(f'{feat1}_x_{feat2}')\n",
        "    print(f\"   ‚úÖ Created: {feat1}_x_{feat2}\")\n",
        "\n",
        "if len(top_5_for_interaction) >= 3:\n",
        "    # Interaction 2: Feature 1 and Feature 3\n",
        "    feat1, feat3 = top_5_for_interaction[0], top_5_for_interaction[2]\n",
        "    X_enhanced[f'{feat1}_x_{feat3}'] = X_scaled[feat1] * X_scaled[feat3]\n",
        "    interactions_created.append(f'{feat1}_x_{feat3}')\n",
        "    print(f\"   ‚úÖ Created: {feat1}_x_{feat3}\")\n",
        "\n",
        "# Create polynomial features for top feature (squared term)\n",
        "if len(top_5_for_interaction) >= 1:\n",
        "    top_feat = top_5_for_interaction[0]\n",
        "    X_enhanced[f'{top_feat}_squared'] = X_scaled[top_feat] ** 2\n",
        "    interactions_created.append(f'{top_feat}_squared')\n",
        "    print(f\"   ‚úÖ Created: {top_feat}_squared\")\n",
        "\n",
        "print(f\"\\n‚úÖ Created {len(interactions_created)} interaction features\")\n",
        "print(f\"   Total features now: {X_enhanced.shape[1]} (was {X_scaled.shape[1]})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test if interaction features improve model performance\n",
        "print(\"üß™ Testing enhanced features with interaction terms...\")\n",
        "\n",
        "lr_enhanced = LogisticRegression(\n",
        "    penalty='l2', C=0.5, class_weight='balanced',\n",
        "    max_iter=2000, random_state=42\n",
        ")\n",
        "\n",
        "lr_enhanced_scores = cross_validate(\n",
        "    lr_enhanced, X_enhanced, y, groups=groups,\n",
        "    cv=gkf, scoring=scoring, return_train_score=True\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä Enhanced Model (with interactions) Results:\")\n",
        "for metric in ['test_accuracy', 'test_recall', 'test_precision', 'test_f1', 'test_roc_auc']:\n",
        "    scores = lr_enhanced_scores[metric]\n",
        "    print(f\"   {metric.replace('test_', '').upper()}: {scores.mean():.3f} ¬± {scores.std():.3f}\")\n",
        "\n",
        "print(f\"\\nüìä Original Model (without interactions) Results:\")\n",
        "for metric in ['test_accuracy', 'test_recall', 'test_precision', 'test_f1', 'test_roc_auc']:\n",
        "    scores = lr_scores[metric]\n",
        "    print(f\"   {metric.replace('test_', '').upper()}: {scores.mean():.3f} ¬± {scores.std():.3f}\")\n",
        "\n",
        "# Compare\n",
        "recall_improvement = lr_enhanced_scores['test_recall'].mean() - lr_scores['test_recall'].mean()\n",
        "auc_improvement = lr_enhanced_scores['test_roc_auc'].mean() - lr_scores['test_roc_auc'].mean()\n",
        "\n",
        "print(f\"\\nüìà Improvement:\")\n",
        "print(f\"   Recall: {recall_improvement:+.3f}\")\n",
        "print(f\"   ROC-AUC: {auc_improvement:+.3f}\")\n",
        "\n",
        "if recall_improvement > 0.01 or auc_improvement > 0.01:\n",
        "    print(f\"\\n‚úÖ Enhanced features show improvement! Consider using them.\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è  Enhanced features show minimal improvement. Original features may be sufficient.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
