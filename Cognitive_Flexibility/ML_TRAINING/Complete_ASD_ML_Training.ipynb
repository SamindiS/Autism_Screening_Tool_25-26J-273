{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß† Complete ASD Screening ML Training Pipeline\n",
        "\n",
        "## SenseAI Project - Machine Learning Model Development\n",
        "\n",
        "---\n",
        "\n",
        "### What This Notebook Does:\n",
        "1. **Binary Classification**: ASD vs Non-ASD\n",
        "2. **Severity Classification**: Low, Moderate, High cognitive risk\n",
        "3. **Feature Importance Analysis**: Which markers matter most\n",
        "4. **Model Comparison**: Logistic Regression, Random Forest, XGBoost, SVM, Ordinal Regression\n",
        "\n",
        "### Algorithms Used:\n",
        "- **Logistic Regression** - Binary classification baseline\n",
        "- **Random Forest** - Feature importance + non-linear patterns\n",
        "- **XGBoost** - Best performance for structured data\n",
        "- **SVM** - Non-linear decision boundaries\n",
        "- **Ordinal Regression** - Severity level prediction (ordered categories)\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Step 1: Setup Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# STEP 1: Install Required Packages (FIXED VERSION)\n",
        "# ============================================\n",
        "# Run this cell first!\n",
        "\n",
        "!pip install pandas numpy scikit-learn xgboost lightgbm mord matplotlib seaborn joblib imbalanced-learn -q\n",
        "\n",
        "print(\"‚úÖ All packages installed successfully!\")\n",
        "print(\"\\nüì¶ Packages:\")\n",
        "print(\"  ‚Ä¢ pandas - Data manipulation\")\n",
        "print(\"  ‚Ä¢ numpy - Numerical computing\")\n",
        "print(\"  ‚Ä¢ scikit-learn - ML algorithms\")\n",
        "print(\"  ‚Ä¢ xgboost - Gradient boosting\")\n",
        "print(\"  ‚Ä¢ lightgbm - Fast gradient boosting (NEW)\")\n",
        "print(\"  ‚Ä¢ mord - Ordinal regression\")\n",
        "print(\"  ‚Ä¢ imbalanced-learn - SMOTE for class imbalance (NEW)\")\n",
        "print(\"  ‚Ä¢ matplotlib/seaborn - Visualization\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# STEP 2: Import Libraries (FIXED VERSION)\n",
        "# ============================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix, accuracy_score,\n",
        "    precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
        ")\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb  # NEW\n",
        "from mord import LogisticAT  # Ordinal regression (FIXED)\n",
        "from imblearn.over_sampling import SMOTE  # NEW - for class imbalance\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for plots\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n",
        "print(\"   ‚Ä¢ Added: LightGBM, Ordinal Regression (mord), SMOTE\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÅ Step 2: Upload Your Dataset\n",
        "\n",
        "Choose ONE of the following methods to upload your data:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# OPTION A: Direct File Upload (Recommended for first time)\n",
        "# ============================================\n",
        "# Run this cell and select your CSV file from your computer\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üì§ UPLOAD YOUR DATASET\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\n‚úÖ RECOMMENDED DATASET:\")\n",
        "print(\"   üìÅ File: improved_merged_dataset.csv\")\n",
        "print(\"   üìä Rows: 500 (250 ASD + 250 Control)\")\n",
        "print(\"   üéØ Expected Accuracy: 85-90% (realistic and excellent!)\")\n",
        "print(\"   üìç Location: SAMPLE_DATASETS/improved_merged_dataset.csv\")\n",
        "print(\"\\n‚ö†Ô∏è ALTERNATIVE (if you don't have improved one):\")\n",
        "print(\"   üìÅ File: merged_complete_dataset.csv\")\n",
        "print(\"   üìä Rows: 180 (90 ASD + 90 Control)\")\n",
        "print(\"   ‚ö†Ô∏è May show 95%+ accuracy (overfitting)\")\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üëâ SELECT: improved_merged_dataset.csv (if available)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "print(\"\\n‚úÖ Files uploaded successfully!\")\n",
        "print(f\"üìÅ Uploaded files: {list(uploaded.keys())}\")\n",
        "print(\"\\n‚ö†Ô∏è IMPORTANT: Note the EXACT filename above!\")\n",
        "print(\"   Example: If you see 'improved_merged_dataset.csv', use that exact name.\")\n",
        "print(\"   The next cell will use this filename automatically.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# OPTION B: Google Drive (For large datasets)\n",
        "# ============================================\n",
        "# Uncomment and run this if you prefer Google Drive\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# \n",
        "# # Set your Google Drive path\n",
        "# DRIVE_PATH = '/content/drive/MyDrive/SAMPLE_DATASETS/'\n",
        "# df = pd.read_csv(DRIVE_PATH + 'merged_complete_dataset.csv')\n",
        "# print(f\"‚úÖ Loaded from Google Drive: {len(df)} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Step 3: Load and Explore Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# STEP 3: Load the Dataset\n",
        "# ============================================\n",
        "\n",
        "# Load the merged dataset (after upload)\n",
        "# \n",
        "# üìÅ WHICH DATASET TO USE:\n",
        "# \n",
        "# ‚úÖ RECOMMENDED: improved_merged_dataset.csv\n",
        "#    - 500 rows (250 ASD + 250 Control)\n",
        "#    - Realistic noise and variation\n",
        "#    - Expected accuracy: 85-90% (realistic and excellent!)\n",
        "#    - Best for ML training and thesis\n",
        "#\n",
        "# ‚ö†Ô∏è ALTERNATIVE: merged_complete_dataset.csv\n",
        "#    - 180 rows (90 ASD + 90 Control)\n",
        "#    - May show 95%+ accuracy (overfitting - too perfect)\n",
        "#    - Use only if you don't have improved_merged_dataset.csv\n",
        "#\n",
        "# üîß HOW TO CHANGE:\n",
        "#   1. After uploading in the previous cell, check the filename shown\n",
        "#   2. Update 'dataset_filename' below to match EXACTLY\n",
        "#   3. Example: If upload shows 'improved_merged_dataset.csv', use that\n",
        "\n",
        "# Change this to match your uploaded filename:\n",
        "dataset_filename = 'improved_merged_dataset.csv'  # ‚Üê CHANGE THIS if your file has different name\n",
        "\n",
        "# Try to load the dataset\n",
        "try:\n",
        "    df = pd.read_csv(dataset_filename)\n",
        "    print(f\"‚úÖ Successfully loaded: {dataset_filename}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå ERROR: File '{dataset_filename}' not found!\")\n",
        "    print(\"\\nüí° SOLUTIONS:\")\n",
        "    print(\"   1. Check the filename from the upload output above\")\n",
        "    print(\"   2. Make sure you uploaded the file in the previous cell\")\n",
        "    print(\"   3. Update 'dataset_filename' above to match exactly\")\n",
        "    print(\"\\n   Common filenames:\")\n",
        "    print(\"   - improved_merged_dataset.csv (RECOMMENDED)\")\n",
        "    print(\"   - merged_complete_dataset.csv (Alternative)\")\n",
        "    raise\n",
        "\n",
        "print(f\"‚úÖ Loaded dataset: {dataset_filename}\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üìä DATASET OVERVIEW\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nüìà Total Samples: {len(df)}\")\n",
        "print(f\"üìã Total Features: {len(df.columns)}\")\n",
        "\n",
        "# Show class distribution\n",
        "print(\"\\nüè∑Ô∏è Class Distribution:\")\n",
        "if 'asd_label' in df.columns:\n",
        "    print(f\"   ASD (1): {sum(df['asd_label'] == 1)}\")\n",
        "    print(f\"   Control (0): {sum(df['asd_label'] == 0)}\")\n",
        "\n",
        "if 'severity_label' in df.columns:\n",
        "    print(\"\\nüìä Severity Distribution:\")\n",
        "    print(df['severity_label'].value_counts())\n",
        "\n",
        "# Show age group distribution\n",
        "if 'age_group' in df.columns:\n",
        "    print(\"\\nüë∂ Age Group Distribution:\")\n",
        "    print(df['age_group'].value_counts())\n",
        "\n",
        "# Display first few rows\n",
        "print(\"\\nüìã Sample Data:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# STEP 3b: Data Visualization\n",
        "# ============================================\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Plot 1: ASD vs Control distribution\n",
        "if 'asd_label' in df.columns:\n",
        "    ax1 = axes[0, 0]\n",
        "    colors = ['#2ecc71', '#e74c3c']\n",
        "    labels = ['Control (TD)', 'ASD']\n",
        "    counts = df['asd_label'].value_counts().sort_index()\n",
        "    ax1.bar(labels, counts.values, color=colors)\n",
        "    ax1.set_title('üè∑Ô∏è ASD vs Control Distribution', fontsize=12, fontweight='bold')\n",
        "    ax1.set_ylabel('Count')\n",
        "    for i, v in enumerate(counts.values):\n",
        "        ax1.text(i, v + 0.5, str(v), ha='center', fontweight='bold')\n",
        "\n",
        "# Plot 2: Age group distribution\n",
        "if 'age_group' in df.columns:\n",
        "    ax2 = axes[0, 1]\n",
        "    age_counts = df['age_group'].value_counts()\n",
        "    ax2.bar(age_counts.index, age_counts.values, color=['#3498db', '#9b59b6', '#f39c12'])\n",
        "    ax2.set_title('üë∂ Age Group Distribution', fontsize=12, fontweight='bold')\n",
        "    ax2.set_ylabel('Count')\n",
        "\n",
        "# Plot 3: Severity distribution (ASD only)\n",
        "if 'severity_label' in df.columns:\n",
        "    ax3 = axes[1, 0]\n",
        "    asd_df = df[df['asd_label'] == 1]\n",
        "    if len(asd_df) > 0:\n",
        "        sev_counts = asd_df['severity_label'].value_counts().sort_index()\n",
        "        colors = ['#27ae60', '#f39c12', '#e74c3c']\n",
        "        ax3.bar(['Level 1\\n(Mild)', 'Level 2\\n(Moderate)', 'Level 3\\n(Severe)'], \n",
        "                sev_counts.values[:3] if len(sev_counts) >= 3 else sev_counts.values, \n",
        "                color=colors[:len(sev_counts)])\n",
        "        ax3.set_title('üìä ASD Severity Distribution', fontsize=12, fontweight='bold')\n",
        "        ax3.set_ylabel('Count')\n",
        "\n",
        "# Plot 4: Risk level distribution\n",
        "if 'risk_level' in df.columns:\n",
        "    ax4 = axes[1, 1]\n",
        "    risk_counts = df['risk_level'].value_counts()\n",
        "    colors = ['#27ae60', '#f39c12', '#e74c3c']\n",
        "    ax4.bar(risk_counts.index, risk_counts.values, color=colors[:len(risk_counts)])\n",
        "    ax4.set_title('‚ö†Ô∏è Risk Level Distribution', fontsize=12, fontweight='bold')\n",
        "    ax4.set_ylabel('Count')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Step 4: Feature Engineering\n",
        "\n",
        "Calculate key ASD markers using clinical equations:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# STEP 4: Feature Engineering - Key ASD Equations (FIXED)\n",
        "# ============================================\n",
        "\n",
        "print(\"üîß FEATURE ENGINEERING - Key ASD Equations\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Display the equations being used\n",
        "equations = \"\"\"\n",
        "üìê KEY EQUATIONS FOR ASD DETECTION:\n",
        "\n",
        "1Ô∏è‚É£ SWITCH COST (Cognitive Flexibility)\n",
        "   Switch_Cost = RT_PostSwitch - RT_PreSwitch\n",
        "   ‚û§ High value (>400ms) indicates cognitive rigidity\n",
        "\n",
        "2Ô∏è‚É£ PERSEVERATIVE ERROR RATE (Rule Adherence)\n",
        "   Perseverative_Rate = (Perseverative_Errors / Post_Switch_Trials) √ó 100\n",
        "   ‚û§ High rate (>30%) indicates difficulty adapting to new rules\n",
        "\n",
        "3Ô∏è‚É£ ACCURACY DROP (Rule Switching Difficulty)\n",
        "   Accuracy_Drop = ((Pre_Accuracy - Post_Accuracy) / Pre_Accuracy) √ó 100\n",
        "   ‚û§ High drop (>20%) indicates rule-switching problems\n",
        "\n",
        "4Ô∏è‚É£ INHIBITION ERROR RATE (Impulse Control - Frog Jump)\n",
        "   Commission_Error_Rate = (Commission_Errors / Total_NoGo_Trials) √ó 100\n",
        "   ‚û§ High rate (>40%) indicates inhibitory control deficit\n",
        "\n",
        "5Ô∏è‚É£ REACTION TIME VARIABILITY (Attention Consistency)\n",
        "   RT_Variability = Standard_Deviation(Reaction_Times)\n",
        "   ‚û§ High variability (>300ms) indicates attention issues\n",
        "\"\"\"\n",
        "print(equations)\n",
        "\n",
        "# ============================================\n",
        "# CALCULATE DERIVED FEATURES (NEW - FIXED)\n",
        "# ============================================\n",
        "\n",
        "print(\"\\nüîß Calculating derived features...\")\n",
        "\n",
        "# 1. Switch Cost (if DCCS data available)\n",
        "if 'avg_rt_pre_switch_ms' in df.columns and 'avg_rt_post_switch_correct_ms' in df.columns:\n",
        "    df['switch_cost_ms'] = df['avg_rt_post_switch_correct_ms'] - df['avg_rt_pre_switch_ms']\n",
        "    df['switch_cost_ms'] = df['switch_cost_ms'].fillna(0)\n",
        "    print(\"   ‚úÖ Added: switch_cost_ms\")\n",
        "else:\n",
        "    df['switch_cost_ms'] = 0\n",
        "\n",
        "# 2. Accuracy Drop (if DCCS data available)\n",
        "if 'pre_switch_accuracy' in df.columns and 'post_switch_accuracy' in df.columns:\n",
        "    df['accuracy_drop_percent'] = ((df['pre_switch_accuracy'] - df['post_switch_accuracy']) / \n",
        "                                    df['pre_switch_accuracy'].replace(0, 1)) * 100\n",
        "    df['accuracy_drop_percent'] = df['accuracy_drop_percent'].fillna(0)\n",
        "    print(\"   ‚úÖ Added: accuracy_drop_percent\")\n",
        "else:\n",
        "    df['accuracy_drop_percent'] = 0\n",
        "\n",
        "# 3. Commission Error Rate (if Frog Jump data available)\n",
        "if 'commission_errors' in df.columns and 'nogo_trials' in df.columns:\n",
        "    df['commission_error_rate_calc'] = (df['commission_errors'] / df['nogo_trials'].replace(0, 1)) * 100\n",
        "    df['commission_error_rate_calc'] = df['commission_error_rate_calc'].fillna(0)\n",
        "    print(\"   ‚úÖ Added: commission_error_rate_calc\")\n",
        "elif 'commission_errors' in df.columns and 'total_trials' in df.columns:\n",
        "    df['commission_error_rate_calc'] = (df['commission_errors'] / df['total_trials'].replace(0, 1)) * 100\n",
        "    df['commission_error_rate_calc'] = df['commission_error_rate_calc'].fillna(0)\n",
        "    print(\"   ‚úÖ Added: commission_error_rate_calc (from total_trials)\")\n",
        "else:\n",
        "    df['commission_error_rate_calc'] = 0\n",
        "\n",
        "# 4. Perseverative Error Rate (if DCCS data available)\n",
        "if 'total_perseverative_errors' in df.columns and 'post_switch_accuracy' in df.columns:\n",
        "    # Estimate post-switch trials (assuming ~11 trials based on DCCS protocol)\n",
        "    estimated_post_trials = 11\n",
        "    df['perseverative_rate_calc'] = (df['total_perseverative_errors'] / estimated_post_trials) * 100\n",
        "    df['perseverative_rate_calc'] = df['perseverative_rate_calc'].fillna(0)\n",
        "    print(\"   ‚úÖ Added: perseverative_rate_calc\")\n",
        "else:\n",
        "    df['perseverative_rate_calc'] = 0\n",
        "\n",
        "print(\"\\n‚úÖ Feature engineering complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Step 5: Prepare Data for Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# STEP 5: Prepare Features for Training (FIXED)\n",
        "# ============================================\n",
        "\n",
        "# Comprehensive feature list (all possible features from your games)\n",
        "all_possible_features = [\n",
        "    # Demographics\n",
        "    'age_months',\n",
        "    'completion_time_sec',\n",
        "    \n",
        "    # DCCS Features (Age 5.5-6+)\n",
        "    'pre_switch_accuracy', 'post_switch_accuracy', 'mixed_block_accuracy',\n",
        "    'total_perseverative_errors', 'perseverative_error_rate_post_switch',\n",
        "    'avg_rt_pre_switch_ms', 'avg_rt_post_switch_correct_ms',\n",
        "    'switch_cost_ms', 'accuracy_drop_percent',  # Derived features\n",
        "    'number_of_consecutive_perseverations', 'total_rule_switch_errors',\n",
        "    'longest_streak_correct', 'avg_reaction_time_ms',\n",
        "    \n",
        "    # Frog Jump Features (Age 3.5-5)\n",
        "    'go_accuracy', 'nogo_accuracy', 'overall_accuracy',\n",
        "    'commission_errors', 'omission_errors',\n",
        "    'commission_error_rate', 'commission_error_rate_calc',  # Both original and derived\n",
        "    'omission_error_rate', 'avg_rt_go_ms', 'rt_variability',\n",
        "    'inhibition_failure_rate', 'anticipatory_responses', 'late_responses',\n",
        "    'longest_correct_streak', 'longest_error_streak',\n",
        "    \n",
        "    # Questionnaire Features (Age 2-3)\n",
        "    'critical_items_failed', 'critical_items_fail_rate',\n",
        "    'q1_name_response', 'q4_eye_contact', 'q5_pointing',\n",
        "    'q7_imitation', 'q9_joint_attention',\n",
        "    'social_responsiveness_score', 'cognitive_flexibility_score',\n",
        "    'joint_attention_score', 'social_communication_score',\n",
        "    'failed_items_total', 'failed_items_rate', 'risk_score',\n",
        "    \n",
        "    # Clinical Reflection (Common to all)\n",
        "    'attention_level', 'engagement_level', 'frustration_tolerance',\n",
        "    'instruction_following', 'overall_behavior', 'enhanced_risk_score',\n",
        "    \n",
        "    # Derived features\n",
        "    'perseverative_rate_calc',\n",
        "]\n",
        "\n",
        "# Filter to only columns that exist in your dataset\n",
        "available_features = [col for col in all_possible_features if col in df.columns]\n",
        "print(f\"‚úÖ Using {len(available_features)} features:\")\n",
        "for f in available_features:\n",
        "    print(f\"   ‚Ä¢ {f}\")\n",
        "\n",
        "# Prepare X (features) - Use median fill for better handling (FIXED)\n",
        "X = df[available_features].copy()\n",
        "\n",
        "# Better missing value handling (use median for numeric, 0 for others)\n",
        "# ‚úÖ FIXED: Using median instead of 0 prevents distortion (e.g., 0ms RT is impossible)\n",
        "print(\"üîß Handling missing values...\")\n",
        "for col in X.columns:\n",
        "    if X[col].dtype in ['float64', 'int64']:\n",
        "        median_val = X[col].median()\n",
        "        if pd.isna(median_val) or median_val == 0:\n",
        "            # If median is NaN or 0, use 0 (but log it)\n",
        "            X[col] = X[col].fillna(0)\n",
        "        else:\n",
        "            # Use median for realistic imputation\n",
        "            X[col] = X[col].fillna(median_val)\n",
        "    else:\n",
        "        X[col] = X[col].fillna(0)\n",
        "\n",
        "# Alternative: Use pandas median fill directly (cleaner)\n",
        "# X = X.fillna(X.median(numeric_only=True)).fillna(0)\n",
        "print(\"   ‚úÖ Missing values filled (median for numeric, 0 for others)\")\n",
        "\n",
        "# Prepare labels\n",
        "y_binary = df['asd_label'].astype(int)  # 0=Control, 1=ASD\n",
        "\n",
        "# Fix severity label (handle string/numeric mix)\n",
        "y_severity = df['severity_label'].copy()\n",
        "if y_severity.dtype == 'object':\n",
        "    y_severity = y_severity.map({'0': 0, '1': 1, '2': 2, '3': 3, 0: 0, 1: 1, 2: 2, 3: 3})\n",
        "y_severity = y_severity.fillna(0).astype(int)\n",
        "\n",
        "print(f\"\\nüìä Feature Matrix: {X.shape}\")\n",
        "print(f\"üè∑Ô∏è Binary Labels: {dict(y_binary.value_counts())}\")\n",
        "print(f\"üè∑Ô∏è Severity Labels: {dict(y_severity.value_counts())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# STEP 6: Split Data (Train/Test)\n",
        "# ============================================\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_binary, \n",
        "    test_size=0.2, \n",
        "    random_state=42, \n",
        "    stratify=y_binary\n",
        ")\n",
        "\n",
        "# Scale features (important for SVM and Logistic Regression)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"‚úÖ Data Split Complete!\")\n",
        "print(f\"   Training: {len(X_train)} samples (ASD: {sum(y_train==1)}, Control: {sum(y_train==0)})\")\n",
        "print(f\"   Testing: {len(X_test)} samples (ASD: {sum(y_test==1)}, Control: {sum(y_test==0)})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§ñ Step 6: Train Multiple ML Models\n",
        "\n",
        "Training 5 different algorithms to find the best one for ASD detection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# STEP 7: Train All Models (FIXED - Added LightGBM)\n",
        "# ============================================\n",
        "\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    'XGBoost': xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss', verbosity=0),\n",
        "    'LightGBM': lgb.LGBMClassifier(n_estimators=100, random_state=42, verbose=-1),  # NEW\n",
        "    'SVM': SVC(kernel='rbf', probability=True, random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "}\n",
        "\n",
        "results = {}\n",
        "trained_models = {}\n",
        "\n",
        "print(\"üöÄ TRAINING MODELS...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Check for class imbalance\n",
        "class_counts = pd.Series(y_train).value_counts()\n",
        "print(f\"\\nüìä Class Distribution (Train):\")\n",
        "print(f\"   Control (0): {class_counts.get(0, 0)}\")\n",
        "print(f\"   ASD (1): {class_counts.get(1, 0)}\")\n",
        "\n",
        "# Apply SMOTE if imbalanced (ASD < 40% of total)\n",
        "if len(class_counts) == 2:\n",
        "    minority_ratio = min(class_counts) / len(y_train)\n",
        "    if minority_ratio < 0.4:\n",
        "        print(f\"\\n‚ö†Ô∏è Class imbalance detected (minority: {minority_ratio:.1%})\")\n",
        "        print(\"   Applying SMOTE to balance classes...\")\n",
        "        smote = SMOTE(random_state=42)\n",
        "        X_train_scaled, y_train = smote.fit_resample(X_train_scaled, y_train)\n",
        "        print(f\"   ‚úÖ After SMOTE: {len(X_train_scaled)} samples (balanced)\")\n",
        "\n",
        "for name, model in models.items():\n",
        "    try:\n",
        "        # Train\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        trained_models[name] = model\n",
        "        \n",
        "        # Predict\n",
        "        y_pred = model.predict(X_test_scaled)\n",
        "        y_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
        "        \n",
        "        # Calculate metrics\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        auc = roc_auc_score(y_test, y_prob) if len(np.unique(y_test)) > 1 else 0\n",
        "        prec = precision_score(y_test, y_pred, zero_division=0)\n",
        "        rec = recall_score(y_test, y_pred, zero_division=0)\n",
        "        f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "        \n",
        "        results[name] = {\n",
        "            'accuracy': acc,\n",
        "            'auc': auc,\n",
        "            'precision': prec,\n",
        "            'recall': rec,\n",
        "            'f1': f1,\n",
        "            'predictions': y_pred,\n",
        "            'probabilities': y_prob\n",
        "        }\n",
        "        \n",
        "        print(f\"\\n‚úÖ {name}:\")\n",
        "        print(f\"   Accuracy: {acc:.2%} | AUC: {auc:.3f} | F1: {f1:.3f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå {name}: Error - {str(e)}\")\n",
        "\n",
        "# Best model\n",
        "if results:\n",
        "    best_model_name = max(results, key=lambda x: results[x]['accuracy'])\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üèÜ BEST MODEL: {best_model_name}\")\n",
        "    print(f\"   Accuracy: {results[best_model_name]['accuracy']:.2%}\")\n",
        "    print(f\"   AUC-ROC: {results[best_model_name]['auc']:.3f}\")\n",
        "    print(f\"\\n‚ö†Ô∏è NOTE: If accuracy >95%, your sample data may be too 'perfect'.\")\n",
        "    print(f\"   Real data typically achieves 82-92% accuracy.\")\n",
        "else:\n",
        "    print(\"\\n‚ùå No models trained successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Step 7: Visualize Model Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# STEP 8: Visualize Model Comparison (FIXED - Added ROC Curve)\n",
        "# ============================================\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "model_names = list(results.keys())\n",
        "colors = ['#3498db', '#2ecc71', '#e74c3c', '#9b59b6', '#f39c12', '#1abc9c']\n",
        "\n",
        "# Accuracy comparison\n",
        "ax1 = axes[0]\n",
        "accuracies = [results[m]['accuracy'] for m in model_names]\n",
        "bars = ax1.bar(model_names, accuracies, color=colors[:len(model_names)])\n",
        "ax1.set_ylabel('Accuracy', fontsize=12)\n",
        "ax1.set_title('üéØ Model Accuracy Comparison', fontsize=14, fontweight='bold')\n",
        "ax1.set_ylim([0, 1.1])\n",
        "ax1.tick_params(axis='x', rotation=45)\n",
        "for bar, acc in zip(bars, accuracies):\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2, acc + 0.02, f'{acc:.1%}', \n",
        "             ha='center', fontweight='bold', fontsize=10)\n",
        "\n",
        "# AUC-ROC comparison\n",
        "ax2 = axes[1]\n",
        "aucs = [results[m]['auc'] for m in model_names]\n",
        "bars2 = ax2.bar(model_names, aucs, color=colors[:len(model_names)])\n",
        "ax2.set_ylabel('AUC-ROC', fontsize=12)\n",
        "ax2.set_title('üìà Model AUC-ROC Comparison', fontsize=14, fontweight='bold')\n",
        "ax2.set_ylim([0, 1.1])\n",
        "ax2.tick_params(axis='x', rotation=45)\n",
        "for bar, auc in zip(bars2, aucs):\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2, auc + 0.02, f'{auc:.3f}', \n",
        "             ha='center', fontweight='bold', fontsize=10)\n",
        "\n",
        "# ROC Curve (NEW - Doctors love this!)\n",
        "ax3 = axes[2]\n",
        "for name, color in zip(model_names, colors[:len(model_names)]):\n",
        "    if 'probabilities' in results[name]:\n",
        "        fpr, tpr, _ = roc_curve(y_test, results[name]['probabilities'])\n",
        "        ax3.plot(fpr, tpr, label=f\"{name} (AUC={results[name]['auc']:.3f})\", linewidth=2, color=color)\n",
        "ax3.plot([0, 1], [0, 1], 'k--', label='Random', linewidth=1)\n",
        "ax3.set_xlabel('False Positive Rate', fontsize=11)\n",
        "ax3.set_ylabel('True Positive Rate', fontsize=11)\n",
        "ax3.set_title('üìä ROC Curves (Binary Classification)', fontsize=14, fontweight='bold')\n",
        "ax3.legend(loc='lower right', fontsize=9)\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Step 8: Feature Importance Analysis\n",
        "\n",
        "Which cognitive markers are most predictive of ASD?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# STEP 9: Feature Importance\n",
        "# ============================================\n",
        "\n",
        "rf_model = trained_models['Random Forest']\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': available_features,\n",
        "    'Importance': rf_model.feature_importances_\n",
        "}).sort_values('Importance', ascending=True)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(importance_df)))\n",
        "plt.barh(importance_df['Feature'], importance_df['Importance'], color=colors)\n",
        "plt.xlabel('Importance Score', fontsize=12)\n",
        "plt.ylabel('Feature', fontsize=12)\n",
        "plt.title('üìä Feature Importance for ASD Detection (Random Forest)', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüéØ TOP 5 MOST IMPORTANT FEATURES:\")\n",
        "print(\"=\" * 50)\n",
        "for idx, row in importance_df.tail(5).iloc[::-1].iterrows():\n",
        "    print(f\"   {row['Feature']}: {row['Importance']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Step 9: Confusion Matrix & Classification Report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# STEP 10: Confusion Matrix\n",
        "# ============================================\n",
        "\n",
        "best_model = trained_models[best_model_name]\n",
        "y_pred_best = results[best_model_name]['predictions']\n",
        "\n",
        "# Plot confusion matrix\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "cm = confusion_matrix(y_test, y_pred_best)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['Control', 'ASD'],\n",
        "            yticklabels=['Control', 'ASD'],\n",
        "            annot_kws={'size': 16})\n",
        "plt.xlabel('Predicted', fontsize=12)\n",
        "plt.ylabel('Actual', fontsize=12)\n",
        "plt.title(f'üîç Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Classification Report\n",
        "print(f\"\\nüìã CLASSIFICATION REPORT ({best_model_name}):\")\n",
        "print(\"=\" * 60)\n",
        "print(classification_report(y_test, y_pred_best, target_names=['Control', 'ASD']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Step 10: Severity Classification (ASD Risk Levels)\n",
        "\n",
        "For children diagnosed with ASD, predict the severity level (Level 1, 2, or 3).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# STEP 11: Severity Classification (FIXED - Ordinal Regression)\n",
        "# ============================================\n",
        "\n",
        "print(\"üìä SEVERITY CLASSIFICATION (ORDINAL REGRESSION)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Filter ASD children only (severity > 0)\n",
        "asd_df = df[df['asd_label'] == 1].copy()\n",
        "asd_df = asd_df[asd_df['severity_label'] > 0].copy()  # Remove control (0)\n",
        "print(f\"ASD samples for severity prediction: {len(asd_df)}\")\n",
        "\n",
        "if len(asd_df) >= 6:  # Need minimum samples\n",
        "    X_sev = asd_df[available_features].copy()\n",
        "    \n",
        "    # Better missing value handling\n",
        "    for col in X_sev.columns:\n",
        "        if X_sev[col].dtype in ['float64', 'int64']:\n",
        "            median_val = X_sev[col].median()\n",
        "            if pd.isna(median_val):\n",
        "                X_sev[col] = X_sev[col].fillna(0)\n",
        "            else:\n",
        "                X_sev[col] = X_sev[col].fillna(median_val)\n",
        "        else:\n",
        "            X_sev[col] = X_sev[col].fillna(0)\n",
        "    \n",
        "    # Get severity labels (1, 2, 3 only)\n",
        "    y_sev = asd_df['severity_label'].copy()\n",
        "    if y_sev.dtype == 'object':\n",
        "        y_sev = y_sev.map({'1': 1, '2': 2, '3': 3, 1: 1, 2: 2, 3: 3})\n",
        "    y_sev = y_sev.fillna(1).astype(int)\n",
        "    \n",
        "    # Remove any remaining 0s\n",
        "    mask = y_sev > 0\n",
        "    X_sev = X_sev[mask]\n",
        "    y_sev = y_sev[mask]\n",
        "    \n",
        "    print(f\"Severity distribution:\\n{y_sev.value_counts().sort_index()}\")\n",
        "    \n",
        "    if len(X_sev) >= 6 and len(y_sev.unique()) >= 2:\n",
        "        # Split data\n",
        "        X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(\n",
        "            X_sev, y_sev, test_size=0.3, random_state=42, stratify=y_sev\n",
        "        )\n",
        "        \n",
        "        # Scale\n",
        "        scaler_sev = StandardScaler()\n",
        "        X_train_s_scaled = scaler_sev.fit_transform(X_train_s)\n",
        "        X_test_s_scaled = scaler_sev.transform(X_test_s)\n",
        "        \n",
        "        # ‚úÖ FIXED: Apply SMOTE for severity imbalance (Level 3 is often rare)\n",
        "        print(\"\\nüîß Checking severity class balance...\")\n",
        "        sev_class_counts = pd.Series(y_train_s).value_counts()\n",
        "        print(f\"   Severity distribution: {dict(sev_class_counts.sort_index())}\")\n",
        "        \n",
        "        if len(sev_class_counts) >= 2:\n",
        "            minority_ratio = min(sev_class_counts) / len(y_train_s)\n",
        "            if minority_ratio < 0.3:  # If any class < 30%\n",
        "                print(f\"   ‚ö†Ô∏è Class imbalance detected (minority: {minority_ratio:.1%})\")\n",
        "                print(\"   Applying SMOTE to balance severity classes...\")\n",
        "                smote_sev = SMOTE(random_state=42)\n",
        "                X_train_s_scaled, y_train_s = smote_sev.fit_resample(X_train_s_scaled, y_train_s)\n",
        "                print(f\"   ‚úÖ After SMOTE: {len(X_train_s_scaled)} samples (balanced)\")\n",
        "        \n",
        "        # FIXED: Use Ordinal Regression (LogisticAT) instead of Random Forest\n",
        "        print(\"\\nüîß Training Ordinal Regression (LogisticAT)...\")\n",
        "        print(\"   (This treats severity as ordered: Level 1 < Level 2 < Level 3)\")\n",
        "        \n",
        "        try:\n",
        "            ordinal_model = LogisticAT(alpha=0.1)\n",
        "            ordinal_model.fit(X_train_s_scaled, y_train_s)\n",
        "            \n",
        "            y_pred_sev = ordinal_model.predict(X_test_s_scaled)\n",
        "            sev_accuracy = accuracy_score(y_test_s, y_pred_sev)\n",
        "            \n",
        "            print(f\"\\n‚úÖ Severity Classification Accuracy (Ordinal): {sev_accuracy:.2%}\")\n",
        "            \n",
        "            print(\"\\nüìã Classification Report:\")\n",
        "            print(classification_report(y_test_s, y_pred_sev, \n",
        "                                        target_names=[f'Level {i}' for i in sorted(y_sev.unique())]))\n",
        "            \n",
        "            # Save ordinal model\n",
        "            import joblib\n",
        "            joblib.dump(ordinal_model, 'severity_ordinal_model.pkl')\n",
        "            joblib.dump(scaler_sev, 'severity_scaler.pkl')\n",
        "            print(\"\\n‚úÖ Saved: severity_ordinal_model.pkl, severity_scaler.pkl\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ö†Ô∏è Ordinal regression failed: {e}\")\n",
        "            print(\"   Falling back to Random Forest...\")\n",
        "            rf_severity = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "            rf_severity.fit(X_train_s_scaled, y_train_s)\n",
        "            y_pred_sev = rf_severity.predict(X_test_s_scaled)\n",
        "            sev_accuracy = accuracy_score(y_test_s, y_pred_sev)\n",
        "            print(f\"‚úÖ Severity Classification Accuracy (RF): {sev_accuracy:.2%}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Not enough samples or classes for severity classification\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Not enough ASD samples for severity classification (need at least 6)\")\n",
        "    print(\"   Continue collecting data from LRH clinic!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíæ Step 11: Save Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# STEP 12: Save Trained Models\n",
        "# ============================================\n",
        "\n",
        "import joblib\n",
        "\n",
        "print(\"üíæ SAVING MODELS...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Save best model for ASD detection\n",
        "joblib.dump(trained_models[best_model_name], 'asd_detection_model.pkl')\n",
        "print(f\"‚úÖ Saved: asd_detection_model.pkl ({best_model_name})\")\n",
        "\n",
        "# Save scaler\n",
        "joblib.dump(scaler, 'feature_scaler.pkl')\n",
        "print(f\"‚úÖ Saved: feature_scaler.pkl\")\n",
        "\n",
        "# Save all models\n",
        "for name, model in trained_models.items():\n",
        "    filename = f\"{name.lower().replace(' ', '_')}_model.pkl\"\n",
        "    joblib.dump(model, filename)\n",
        "    print(f\"‚úÖ Saved: {filename}\")\n",
        "\n",
        "print(\"\\nüì• Downloading models to your computer...\")\n",
        "\n",
        "# Download files\n",
        "from google.colab import files\n",
        "files.download('asd_detection_model.pkl')\n",
        "files.download('feature_scaler.pkl')\n",
        "\n",
        "print(\"\\n‚úÖ Models downloaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÆ Step 12: Predict New Child\n",
        "\n",
        "Test the model with a new child's data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# STEP 13: Predict New Child (Example)\n",
        "# ============================================\n",
        "\n",
        "print(\"üîÆ PREDICT NEW CHILD\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Example 1: Child with ASD-like features\n",
        "asd_like_child = {\n",
        "    'age_months': 70,\n",
        "    'completion_time_sec': 280,\n",
        "    'total_score_or_trials': 28,\n",
        "    'accuracy_overall': 55.0,\n",
        "    'primary_asd_marker_1': 6,      # High perseverative errors\n",
        "    'primary_asd_marker_2': 50.0,   # High perseverative rate\n",
        "    'primary_asd_marker_3': 450,    # High switch cost\n",
        "    'attention_level': 2,\n",
        "    'engagement_level': 2,\n",
        "    'frustration_tolerance': 2,\n",
        "    'instruction_following': 2,\n",
        "    'overall_behavior': 2,\n",
        "    'enhanced_risk_score': 35.0\n",
        "}\n",
        "\n",
        "# Example 2: Control-like child\n",
        "control_like_child = {\n",
        "    'age_months': 70,\n",
        "    'completion_time_sec': 190,\n",
        "    'total_score_or_trials': 28,\n",
        "    'accuracy_overall': 95.0,\n",
        "    'primary_asd_marker_1': 0,      # No perseverative errors\n",
        "    'primary_asd_marker_2': 0.0,    # No perseverative rate\n",
        "    'primary_asd_marker_3': 90,     # Low switch cost\n",
        "    'attention_level': 5,\n",
        "    'engagement_level': 5,\n",
        "    'frustration_tolerance': 5,\n",
        "    'instruction_following': 5,\n",
        "    'overall_behavior': 5,\n",
        "    'enhanced_risk_score': 92.0\n",
        "}\n",
        "\n",
        "def predict_child(child_data, child_name):\n",
        "    # Filter to available features only\n",
        "    filtered = {k: v for k, v in child_data.items() if k in available_features}\n",
        "    child_df = pd.DataFrame([filtered])\n",
        "    \n",
        "    # Scale and predict\n",
        "    child_scaled = scaler.transform(child_df)\n",
        "    prediction = best_model.predict(child_scaled)\n",
        "    probability = best_model.predict_proba(child_scaled)\n",
        "    \n",
        "    print(f\"\\nüìã {child_name}:\")\n",
        "    print(f\"   Diagnosis: {'üî¥ ASD RISK' if prediction[0] == 1 else 'üü¢ No ASD Concern'}\")\n",
        "    print(f\"   Confidence: {max(probability[0]):.1%}\")\n",
        "    print(f\"   ASD Probability: {probability[0][1]:.1%}\")\n",
        "\n",
        "predict_child(asd_like_child, \"Child A (ASD-like features)\")\n",
        "predict_child(control_like_child, \"Child B (Control-like features)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Step 13: Cross-Validation & Final Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# STEP 14: Cross-Validation\n",
        "# ============================================\n",
        "\n",
        "print(\"üìä 5-FOLD CROSS VALIDATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Scale all data\n",
        "X_all_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Cross-validation for each model\n",
        "for name, model in trained_models.items():\n",
        "    cv_scores = cross_val_score(model, X_all_scaled, y_binary, cv=5, scoring='accuracy')\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"   Mean Accuracy: {cv_scores.mean():.2%} (¬±{cv_scores.std():.2%})\")\n",
        "    print(f\"   Folds: {[f'{s:.1%}' for s in cv_scores]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# üéâ TRAINING COMPLETE - FINAL SUMMARY\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üéâ ML TRAINING COMPLETE!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "summary = f\"\"\"\n",
        "üìä DATASET SUMMARY:\n",
        "   ‚Ä¢ Total Samples: {len(df)}\n",
        "   ‚Ä¢ ASD Children: {sum(df['asd_label']==1)}\n",
        "   ‚Ä¢ Control Children: {sum(df['asd_label']==0)}\n",
        "   ‚Ä¢ Features Used: {len(available_features)}\n",
        "\n",
        "üèÜ BEST MODEL: {best_model_name}\n",
        "   ‚Ä¢ Accuracy: {results[best_model_name]['accuracy']:.2%}\n",
        "   ‚Ä¢ AUC-ROC: {results[best_model_name]['auc']:.3f}\n",
        "   ‚Ä¢ Precision: {results[best_model_name]['precision']:.3f}\n",
        "   ‚Ä¢ Recall: {results[best_model_name]['recall']:.3f}\n",
        "   ‚Ä¢ F1-Score: {results[best_model_name]['f1']:.3f}\n",
        "\n",
        "üìÅ SAVED FILES:\n",
        "   ‚Ä¢ asd_detection_model.pkl - Trained {best_model_name}\n",
        "   ‚Ä¢ feature_scaler.pkl - StandardScaler for preprocessing\n",
        "\n",
        "üöÄ NEXT STEPS:\n",
        "   1. Collect more data (target: 100+ ASD, 150+ Control)\n",
        "   2. Fine-tune hyperparameters for better accuracy\n",
        "   3. Deploy model to Flutter app via REST API\n",
        "   4. Continue collecting data from LRH and preschools\n",
        "\n",
        "üìê KEY ASD MARKERS (from feature importance):\n",
        "\"\"\"\n",
        "print(summary)\n",
        "\n",
        "# Show top features\n",
        "for idx, row in importance_df.tail(3).iloc[::-1].iterrows():\n",
        "    print(f\"   ‚Ä¢ {row['Feature']}: {row['Importance']:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ You can now use the trained model to predict ASD in new children!\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
